{#-

  Jinja2 Template for Makefile

  This file is a template that generates the Makefile.
  This comment will not be included.

  See: http://jinja.pocoo.org/docs/2.10/templates

-#}
# ----------------------------------------
# Makefile for {{ project.id }}
# Generated using ontology-development-kit
# ODK Version: {% if env is defined %}{{env['ODK_VERSION'] or "Unknown" }}{% else %}"Unknown"{% endif %}
# ----------------------------------------
# IMPORTANT: DO NOT EDIT THIS FILE. To override default make goals, use {{ project.id }}.Makefile instead

{{ project.custom_makefile_header }}

# ----------------------------------------
# Standard Constants
# ----------------------------------------
# these can be overwritten on the command line

URIBASE=                    {{ project.uribase|default('http://purl.obolibrary.org/obo') }}
ONT=                        {{ project.id }}
ONTBASE=                    $(URIBASE)/$(ONT)
EDIT_FORMAT=                {{ project.edit_format|default('owl') }}
SRC =                       $(ONT)-edit.$(EDIT_FORMAT)
CATALOG=                    {{ project.catalog_file }}
ROBOT=                      robot --catalog $(CATALOG)
{% if project.owltools_memory|length %}OWLTOOLS_MEMORY=            {{ project.owltools_memory }}{% endif %}
OWLTOOLS=                   {% if project.owltools_memory|length %}OWLTOOLS_MEMORY=$(OWLTOOLS_MEMORY) {% endif %}owltools --use-catalog
RELEASEDIR=                 ../..
REPORTDIR=                  reports
TEMPLATEDIR=                ../templates
TMPDIR=                     tmp
SCRIPTSDIR=                 ../scripts
SPARQLDIR =                 ../sparql
COMPONENTSDIR =             {{ project.components.directory|default('components') }}
{%- if project.robot_report.custom_profile %}
ROBOT_PROFILE =             profile.txt
{%- endif %}
REPORT_FAIL_ON =            {{ project.robot_report.fail_on|default('ERROR') }}
REPORT_LABEL =              {% if project.robot_report.use_labels %}-l true{% endif %}
REPORT_PROFILE_OPTS =       {% if project.robot_report.custom_profile %}--profile $(ROBOT_PROFILE){% endif %}
OBO_FORMAT_OPTIONS =        {{ project.obo_format_options }}
SPARQL_VALIDATION_CHECKS =  {% if project.robot_report.custom_sparql_checks is defined %}{% for x in project.robot_report.custom_sparql_checks %} {{ x }}{% endfor %}{% endif %}
SPARQL_EXPORTS =            {% if project.robot_report.custom_sparql_exports is defined %}{% for x in project.robot_report.custom_sparql_exports %} {{ x }}{% endfor %}{% endif %}
ODK_VERSION_MAKEFILE =      {% if env is defined %}{{env['ODK_VERSION'] or "Unknown" }}{% else %}"Unknown"{% endif %}

TODAY ?=                    $(shell date +%Y-%m-%d)
OBODATE ?=                  $(shell date +'%d:%m:%Y %H:%M')
VERSION=                    $(TODAY)
ANNOTATE_ONTOLOGY_VERSION = annotate -V $(ONTBASE)/releases/$(VERSION)/$@ --annotation owl:versionInfo $(VERSION)
OTHER_SRC =                 {% if project.use_dosdps -%}$(PATTERNDIR)/definitions.owl {% endif -%}{% if project.components is defined -%}{% for component in project.components.products -%}$(COMPONENTSDIR)/{{ component.filename }} {% endfor -%}{% endif %}
ONTOLOGYTERMS =             $(TMPDIR)/ontologyterms.txt

{%- if project.use_dosdps %}
PATTERNDIR=                 ../patterns
DOSDP_SCHEMA=                http:// # change to PURL when ready.
PATTERN_TESTER=              simple_pattern_tester.py
DOSDPT=                      dosdp-tools
PATTERN_RELEASE_FILES=       $(PATTERNDIR)/definitions.owl $(PATTERNDIR)/pattern.owl

{% endif %}

FORMATS = $(sort {% for format in project.export_formats %} {{ format }}{% endfor %} owl)
FORMATS_INCL_TSV = $(sort $(FORMATS) tsv)
RELEASE_ARTEFACTS = $(sort {% for release in project.release_artefacts %}{% if release.startswith('custom-') %}{{ release | replace("custom-","")}}{% else %}$(ONT)-{{ release }}{% endif %} {% endfor %}$(ONT)-base $(ONT)-full)

# ----------------------------------------
# Top-level targets
# ----------------------------------------

.PHONY: .FORCE

.PHONY: all
all: odkversion all_imports {% if project.use_dosdps %}patterns {% endif %}all_main all_subsets sparql_test all_reports all_assets

.PHONY: test
test: odkversion sparql_test all_reports {% if project.robot_report.ensure_owl2dl_profile %}$(REPORTDIR)/validate_profile_owl2dl_$(ONT).owl.txt{% endif %}
	$(ROBOT) reason --input $(SRC) --reasoner {{ project.reasoner }}  --equivalent-classes-allowed {{ project.allow_equivalents }} --exclude-tautologies {{ project.exclude_tautologies }} --output test.owl && rm test.owl && echo "Success"

.PHONY: odkversion
odkversion:
	echo "ODK Makefile version: $(ODK_VERSION_MAKEFILE) (this is the version of the ODK with which this Makefile was generated, not the version of the ODK you are running)" &&\
	echo "ROBOT version (ODK): " && $(ROBOT) --version

$(TMPDIR) $(REPORTDIR) :
	mkdir -p $@

## -- main targets --
##
## By default this is the cross-product of {ont, ont-base} x FORMATS

MAIN_PRODUCTS = $(sort $(foreach r,$(RELEASE_ARTEFACTS), $(r)) $(ONT))
MAIN_GZIPPED = {% if project.gzip_main %}$(foreach f,$(FORMATS), $(ONT).$(f).gz){% endif %}
MAIN_FILES = $(foreach n,$(MAIN_PRODUCTS), $(foreach f,$(FORMATS), $(n).$(f))) $(MAIN_GZIPPED)

.PHONY: all_main
all_main: $(MAIN_FILES)

## -- import targets --
##
## By default this is the cross-product of IMPORT_MODULES x FORMATS

{% if project.import_group is defined %}
IMPORTS = {% for imp in project.import_group.products %} {{ imp.id }}{% endfor %}
{% else %}
IMPORTS =
{% endif %}
IMPORT_ROOTS = $(patsubst %, imports/%_import, $(IMPORTS))
IMPORT_OWL_FILES = $(foreach n,$(IMPORT_ROOTS), $(n).owl)
{%- if 'obo' == project.edit_format %}
IMPORT_OBO_FILES = $(foreach n,$(IMPORT_ROOTS), $(n).obo)
IMPORT_FILES = $(IMPORT_OWL_FILES) $(IMPORT_OBO_FILES)

.PHONY: normalize_obo_src
normalize_obo_src: $(SRC)
	$(OWLTOOLS) $< --merge-axiom-annotations -o -f obo $(TMPDIR)/NORM.obo && robot convert -i $(TMPDIR)/NORM.obo -o $(TMPDIR)/NORM.tmp.obo && mv $(TMPDIR)/NORM.tmp.obo $(SRC)

.PHONY: normalize_src
normalize_src: $(SRC)
	$(ROBOT) convert -i $< -f {% if 'obo' == project.edit_format %}obo --check false{% else %}ofn{% endif %} -o tmp/normalise && mv tmp/normalise $<

{% else %}
IMPORT_FILES = $(IMPORT_OWL_FILES)
{% endif %}

.PHONY: all_imports
all_imports: $(IMPORT_FILES)

## -- subset targets --
##
## By default this is the cross-product of SUBSETS x FORMATS
## Note we also include TSV as a format

{% if project.subset_group is defined %}
SUBSETS = {% for x in project.subset_group.products %} {{ x.id }}{% endfor %}
{% else %}
SUBSETS =
{% endif %}
SUBSET_ROOTS = $(patsubst %, subsets/%, $(SUBSETS))
SUBSET_FILES = $(foreach n,$(SUBSET_ROOTS), $(foreach f,$(FORMATS_INCL_TSV), $(n).$(f)))

.PHONY: all_subsets
all_subsets: $(SUBSET_FILES)

OBO_REPORT = {% for x in project.robot_report.report_on %} {% if x=="edit" %}$(SRC){% else %}{{ x }}{% endif %}-obo-report{% endfor %}
REPORTS = $(OBO_REPORT)
REPORT_FILES = $(patsubst %, $(REPORTDIR)/%.tsv, $(REPORTS))

.PHONY: robot_reports
robot_reports: $(REPORT_FILES)

.PHONY: all_reports
all_reports: all_reports_onestep $(REPORT_FILES)

$(REPORTDIR)/validate_profile_owl2dl_%.txt: % | $(REPORTDIR)
	$(ROBOT) validate-profile --profile DL -i $< -o $@
.PRECIOUS: $(REPORTDIR)/validate_profile_owl2dl_%.txt

.PHONY: validate_profile_%
validate_profile_%: $(REPORTDIR)/validate_profile_owl2dl_%.txt

## -- all files/assets --

ASSETS = \
  $(IMPORT_FILES) \
  $(MAIN_FILES) \
  $(REPORT_FILES) \
  $(SUBSET_FILES)

RELEASE_ASSETS = \
  $(MAIN_FILES) {% if project.import_group is defined %}{% if project.import_group.release_imports %}$(IMPORT_FILES) {% endif %}{% endif %}{% if project.robot_report.release_reports %}$(REPORT_FILES){% endif -%}\
  $(SUBSET_FILES)

.PHONY: all_assets
all_assets: $(ASSETS)


.PHONY: show_assets
show_assets:
	echo $(ASSETS)
	du -sh $(ASSETS)


# ----------------------------------------
# Release Management
# ----------------------------------------

{% if 'basic' in project.release_artefacts or project.primary_release == 'basic' -%}
KEEPRELATIONS=keeprelations.txt
{% endif -%}

CLEANFILES=$(MAIN_FILES) $(SRCMERGED)
# This should be executed by the release manager whenever time comes to make a release.
# It will ensure that all assets/files are fresh, and will copy to release folder

.PHONY: prepare_release
prepare_release: $(ASSETS) $(PATTERN_RELEASE_FILES)
	rsync -R $(RELEASE_ASSETS) $(RELEASEDIR) &&\
  {% if project.use_dosdps -%}
  mkdir -p $(RELEASEDIR)/patterns &&\
  cp $(PATTERN_RELEASE_FILES) $(RELEASEDIR)/patterns &&\
  {% endif -%}
  rm -f $(CLEANFILES) &&\
  echo "Release files are now in $(RELEASEDIR) - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab"

.PHONY: prepare_initial_release
prepare_initial_release: prepare_release
	cd $(RELEASEDIR) && git add $(RELEASE_ASSETS)

# ----------------------------------------
# Import modules
# ----------------------------------------
# Most ontologies are modularly constructed using portions of other ontologies
# These live in the imports/ folder

# ------------------------
# Imports: Seeding system 
# ------------------------

# seed.txt contains all referenced entities
IMPORTSEED=$(TMPDIR)/seed.txt
SRCMERGED=$(TMPDIR)/merged-$(SRC)
PRESEED=$(TMPDIR)/pre_seed.txt

$(SRCMERGED): $(SRC)
	$(ROBOT) remove --input $< --select imports --trim false \
		merge  $(patsubst %, -i %, $(OTHER_SRC)) -o $@

$(PRESEED): $(SRCMERGED)
	$(ROBOT) query -f csv -i $< --query ../sparql/terms.sparql $@.tmp &&\
	cat $@.tmp | sort | uniq >  $@

{% if 'basic' in project.release_artefacts or 'simple' in project.release_artefacts or project.primary_release == 'basic' or project.primary_release == 'simple' -%}
SIMPLESEED=$(TMPDIR)/simple_seed.txt

$(SIMPLESEED): $(SRCMERGED) $(ONTOLOGYTERMS)
	$(ROBOT) query -f csv -i $< --query ../sparql/simple-seed.sparql $@.tmp &&\
	cat $@.tmp $(ONTOLOGYTERMS) | sort | uniq >  $@ &&\
	echo "http://www.geneontology.org/formats/oboInOwl#SubsetProperty" >> $@ &&\
	echo "http://www.geneontology.org/formats/oboInOwl#SynonymTypeProperty" >> $@
{% endif -%}
{% if project.use_custom_import_module %} 
IMPORT_MODULE_TEMPLATE=$(TEMPLATEDIR)/external_import.tsv
IMPORT_MODULE_SIGNATURE=$(TMPDIR)/external_import_terms.txt
IMPORT_MODULE=imports/external_import.owl
$(IMPORT_MODULE): $(IMPORT_MODULE_TEMPLATE)
	$(ROBOT) template --template $< \
  --ontology-iri "$(ONTBASE)/external_import.owl" \
  --output $@

$(IMPORT_MODULE_SIGNATURE): $(IMPORT_MODULE)
	$(ROBOT) query -f csv -i $< --query ../sparql/terms.sparql $@.tmp &&\
	cat $@.tmp | sort | uniq >  $@
{% endif %}

ALLSEED = $(PRESEED) {% if project.use_dosdps %} $(PATTERNDIR)/all_pattern_terms.txt {% endif %}\
{% if project.use_custom_import_module %} $(IMPORT_MODULE_SIGNATURE) {% endif %}

$(IMPORTSEED): {% if project.use_dosdps %} prepare_patterns {% endif %} $(ALLSEED) 
	if [ $(IMP) = true ]; then cat $(ALLSEED) | sort | uniq > $@; fi


{% if project.import_group is defined -%}
ANNOTATION_PROPERTIES={% for p in project.import_group.annotation_properties %}{{ p }} {% endfor %}

# -- Generate Import Modules --
#
# This pattern uses ROBOT to generate an import module
# Generate terms.txt for each import.  (Assume OBO-style Possibly hacky step?)
# Should be able to drop this if robot can just take a big messy list of terms as input.

ALL_TERMS_COMBINED = $(patsubst %, imports/%_terms_combined.txt, $(IMPORTS))

imports/merged_terms_combined.txt: $(ALL_TERMS_COMBINED)
	if [ $(IMP) = true ]; then cat $^ | grep -v ^# | sort | uniq >  $@; fi

imports/%_terms_pre_combined.txt: $(IMPORTSEED) imports/%_terms.txt
	if [ $(IMP) = true ]; then cat $^ | grep -v ^# | sort | uniq >  $@; fi

{% if 'slme' == project.import_group.module_type %}
imports/%_terms_combined.txt: imports/%_terms_pre_combined.txt {% if project.import_group.use_base_merging %}mirror/merged.owl{% endif %}
	if [ $(IMP) = true ]; then {% if project.import_group.use_base_merging %}$(ROBOT) extract -i mirror/merged.owl -T $< --force true --individuals {{ project.import_group.slme_individuals }} --method {{ project.import_group.module_type_slme }} \
	query --query ../sparql/tbox-signature.sparql $@{% else %}cp $< $@{% endif %}; fi

imports/%_import.owl: mirror/%.owl imports/merged_terms_combined.txt
	if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \
		extract -T imports/$*_terms_combined.txt --force true --copy-ontology-annotations true --individuals {{ project.import_group.slme_individuals }} --method {{ project.import_group.module_type_slme }} \
		query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \
		annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi
{% elif 'minimal' == project.import_group.module_type %}
imports/%_terms_combined.txt: imports/%_terms_pre_combined.txt {% if project.import_group.use_base_merging %}mirror/merged.owl{% endif %}
	if [ $(IMP) = true ]; then {% if project.import_group.use_base_merging %}$(ROBOT) extract -i mirror/merged.owl -T $< --force true --method BOT \
	query --query ../sparql/tbox-signature.sparql $@{% else %}cp $< $@{% endif %}; fi

imports/%_import.owl: mirror/%.owl imports/merged_terms_combined.txt
	if [ $(IMP) = true ]; then $(ROBOT) query  -i $< --update ../sparql/preprocess-module.ru \
		extract -T imports/$*_terms_combined.txt --force true --copy-ontology-annotations true --method BOT \
		remove --base-iri $(URIBASE)"/$(shell echo $* | tr a-z A-Z)_" --axioms external --preserve-structure false --trim false \
		query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \
		remove $(patsubst %, --term %, $(ANNOTATION_PROPERTIES)) -T imports/$*_terms_combined.txt --select complement --select "classes individuals annotation-properties" \
		annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi
{% elif 'mirror' == project.import_group.module_type %}
imports/%_terms_combined.txt: imports/%_terms_pre_combined.txt
	if [ $(IMP) = true ]; then cp $< $@; fi

imports/%_import.owl: mirror/%.owl imports/merged_terms_combined.txt
	if [ $(IMP) = true ]; then $(ROBOT) merge -i $< query --update ../sparql/preprocess-module.ru --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \
		annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi
{% elif 'filter' == project.import_group.module_type %}
imports/%_terms_combined.txt: imports/%_terms_pre_combined.txt
	if [ $(IMP) = true ]; then cp $< $@; fi

imports/%_import.owl: mirror/%.owl imports/merged_terms_combined.txt
	if [ $(IMP) = true ]; then $(ROBOT) merge -i $< \
		query --update ../sparql/preprocess-module.ru \
		remove --base-iri $(URIBASE)"/$(shell echo $* | tr a-z A-Z)_" --axioms external --preserve-structure false --trim false \
		remove $(patsubst %, --term %, $(ANNOTATION_PROPERTIES)) -T imports/$*_terms_combined.txt --select complement \
		query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \
		annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi
{% elif 'custom' == project.import_group.module_type %}
imports/%_import.owl: mirror/%.owl
	echo "ERROR: You have configured your default module type to be custom; this behavior needs to be overwritten in {{ project.id }}.Makefile!" && false
{% endif %}
.PRECIOUS: imports/%_import.owl
{% if 'obo' == project.edit_format %}
# convert imports to obo.
# this can be useful for spot-checks and diffs.
# we set strict mode to false by default. For discussion see https://github.com/owlcs/owlapi/issues/752
imports/%_import.obo: imports/%_import.owl
	if [ $(IMP) = true ]; then $(ROBOT) convert --check false -i $< -f obo -o $@.tmp.obo && mv $@.tmp.obo $@; fi
{% endif -%}

{%- for ont in project.import_group.products -%}
{% if ont.is_large or ont.module_type is not none %}
## Module for ontology: {{ ont.id }}
{% if ont.is_large or ('fast_slme' == ont.module_type) or (ont.module_type is none and 'fast_slme' == project.import_group.module_type) %}
imports/{{ ont.id }}_terms_combined.txt: imports/{{ ont.id }}_terms_pre_combined.txt {% if project.import_group.use_base_merging %}mirror/merged.owl{% endif %}
	if [ $(IMP) = true ]; then {% if project.import_group.use_base_merging %}$(ROBOT) extract -i mirror/merged.owl -T $< --force true --individuals {% if ont.module_type is none %}{{ project.import_group.slme_individuals }} --method {{ project.import_group.module_type_slme }}{% else %}{{ ont.slme_individuals }} --method {{ ont.module_type_slme }}{% endif %} \
	query --query ../sparql/tbox-signature.sparql $@{% else %}cp $< $@{% endif %}; fi

imports/{{ ont.id }}_import.owl: mirror/{{ ont.id }}.owl imports/merged_terms_combined.txt
	if [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then $(ROBOT) extract -i $< -T imports/{{ ont.id }}_terms_combined.txt --force true --individuals {% if ont.module_type is none %}{{ project.import_group.slme_individuals }} --method {{ project.import_group.module_type_slme }}{% else %}{{ ont.slme_individuals }} --method {{ ont.module_type_slme }}{% endif %} \
		query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \
		annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi
{% elif ('slme' == ont.module_type) or (ont.module_type is none and 'slme' == project.import_group.module_type) %}
imports/{{ ont.id }}_terms_combined.txt: imports/%_terms_pre_combined.txt {% if project.import_group.use_base_merging %}mirror/merged.owl{% endif %}
	if [ $(IMP) = true ]; then {% if project.import_group.use_base_merging %}$(ROBOT) extract -i mirror/merged.owl -T $< --force true --individuals {% if ont.module_type is none %}{{ project.import_group.slme_individuals }} --method {{ project.import_group.module_type_slme }}{% else %}{{ ont.slme_individuals }} --method {{ ont.module_type_slme }}{% endif %} \
	query --query ../sparql/tbox-signature.sparql $@{% else %}cp $< $@{% endif %}; fi

imports/{{ ont.id }}_import.owl: mirror/{{ ont.id }}.owl imports/merged_terms_combined.txt
	if [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then $(ROBOT) query  -i $< --update ../sparql/preprocess-module.ru \
		extract -T imports/{{ ont.id }}_terms_combined.txt --force true --individuals {% if ont.module_type is none %}{{ project.import_group.slme_individuals }} --method {{ project.import_group.module_type_slme }}{% else %}{{ ont.slme_individuals }} --method {{ ont.module_type_slme }}{% endif %} \
		query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \
		annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi
{% elif ('filter' == ont.module_type) or (ont.module_type is none and 'filter' == project.import_group.module_type) %}
imports/{{ ont.id }}_terms_combined.txt: imports/{{ ont.id }}_terms_pre_combined.txt
	if [ $(IMP) = true ]; then cp $< $@; fi

imports/{{ ont.id }}_import.owl: mirror/{{ ont.id }}.owl imports/merged_terms_combined.txt
	if [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then $(ROBOT) merge -i $< \
		query --update ../sparql/preprocess-module.ru \
		remove {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{ iri }} {% endfor %}{% else %}--base-iri $(URIBASE)/{{ ont.id.upper() }} {% endif %}--axioms external --preserve-structure false --trim false \
		remove $(patsubst %, --term %, $(ANNOTATION_PROPERTIES)) -T imports/{{ ont.id }}_terms_combined.txt --select complement \
		query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \
		annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi
{% elif ('mirror' == ont.module_type) or (ont.module_type is none and 'mirror' == project.import_group.module_type) %}
imports/{{ ont.id }}_terms_combined.txt: imports/{{ ont.id }}_terms_pre_combined.txt
	if [ $(IMP) = true ]; then cp $< $@; fi

imports/{{ ont.id }}_import.owl: mirror/{{ ont.id }}.owl imports/merged_terms_combined.txt
	if [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then $(ROBOT) merge -i $< \
		query --update ../sparql/preprocess-module.ru --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \
		annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi
{% elif ('minimal' == ont.module_type) or (ont.module_type is none and 'minimal' == project.import_group.module_type) %}
imports/{{ ont.id }}_terms_combined.txt: imports/{{ ont.id }}_terms_pre_combined.txt {% if project.import_group.use_base_merging %}mirror/merged.owl{% endif %}
	if [ $(IMP) = true ]; then {% if project.import_group.use_base_merging %}$(ROBOT) extract -i mirror/merged.owl -T $< --force true --method BOT \
	query --query ../sparql/tbox-signature.sparql $@{% else %}cp $< $@{% endif %}; fi

imports/{{ ont.id }}_import.owl: mirror/{{ ont.id }}.owl imports/merged_terms_combined.txt
	if [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then $(ROBOT) extract  -i $< -T imports/{{ ont.id }}_terms_combined.txt --force true --method BOT \
		remove {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{iri}} {% endfor %}{% else %}--base-iri $(URIBASE)/{{ ont.id.upper() }} {% endif %}--axioms external --preserve-structure false --trim false \
		query --update ../sparql/inject-subset-declaration.ru \
		remove $(patsubst %, --term %, $(ANNOTATION_PROPERTIES)) -T imports/{{ ont.id }}_terms_combined.txt --select complement --select "classes individuals annotation-properties" \
		annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi
{% elif ('custom' == ont.module_type) or (ont.module_type is none and 'custom' == project.import_group.module_type) %}
imports/{{ ont.id }}_import.owl: mirror/{{ ont.id }}.owl
	echo "ERROR: You have configured your default module type to be custom; this behavior needs to be overwritten in {{ project.id }}.Makefile!" && false
{%- endif %}
{%- endif %}
{%- endfor %}
.PHONY: refresh-imports
refresh-imports:
	make IMP=true MIR=true PAT=false IMP_LARGE=true all_imports -B

.PHONY: no-mirror-refresh-imports
no-mirror-refresh-imports:
	make IMP=true MIR=false PAT=false IMP_LARGE=true all_imports -B


.PHONY: refresh-imports-excluding-large
refresh-imports-excluding-large:
	make IMP=true MIR=true PAT=false IMP_LARGE=false all_imports -B

.PHONY: refresh-%
refresh-%:
	make IMP=true IMP_LARGE=true MIR=true PAT=false imports/$*_import.owl -B

.PHONY: no-mirror-refresh-%
no-mirror-refresh-%:
	make IMP=true IMP_LARGE=true MIR=false PAT=false imports/$*_import.owl -B
{%- endif %}

{% if project.components is not none %}
# ----------------------------------------
# Components
# ----------------------------------------
# Some ontologies contain external and internal components. A component is included in the ontology in its entirety.

$(COMPONENTSDIR)/%:
	touch $@
.PRECIOUS: $(COMPONENTSDIR)/%


{% for component in project.components.products %}
{% if component.source is not none %}
$(COMPONENTSDIR)/{{ component.filename }}: .FORCE
	if [ $(IMP) = true ]; then $(ROBOT) merge -I {{ component.source }} \
	annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) -o $@; fi
.PRECIOUS: $(COMPONENTSDIR)/{{ component.filename }}
{% endif %}
{% endfor %}
{% endif %}

# ----------------------------------------
# Mirroring upstream ontologies
# ----------------------------------------
#

IMP=true # Global parameter to bypass import generation
MIR=true # Global parameter to bypass mirror generation
IMP_LARGE=true # Global parameter to bypass handling of large imports
{% if project.use_dosdps %}PAT=true # Global parameter to bypass pattern generation{% endif -%}

{% if project.import_group is defined -%}
{% for ont in project.import_group.products %}

## ONTOLOGY: {{ ont.id }}
{% if ont.description  -%}
## {{ ont.description }}
{% endif -%}
{% if ont.rebuild_if_source_changes -%}
## Copy of {{ont.id}} is re-downloaded whenever source changes
mirror/{{ ont.id }}.trigger: $(SRC)
{% else -%}
## Copy of {{ont.id}} is re-downloaded manually
mirror/{{ ont.id }}.trigger:
	touch $@
{% endif -%}

{%- if ont.mirror_from %}
mirror/{{ ont.id }}.owl: mirror/{{ ont.id }}.trigger
	if [ $(MIR) = true ] && [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then $(ROBOT) convert -I {{ ont.mirror_from }} -o $@.tmp.owl{%- if ont.make_base %} && \
		$(ROBOT) remove -i $@.tmp.owl {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{iri}} {% endfor %}{% else %}--base-iri $(URIBASE)/{{ ont.id.upper() }} {% endif %}--axioms external --preserve-structure false --trim false -o $@.tmp.owl{% endif %} && mv $@.tmp.owl $@; fi
{%- elif ont.use_base %}
{%- if ont.use_gzipped %}
mirror/{{ ont.id }}.owl: mirror/{{ ont.id }}.trigger
	if [ $(MIR) = true ] && [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then curl -L $(URIBASE)/{{ ont.id }}/{{ ont.id }}-base.owl.gz --create-dirs -o mirror/{{ ont.id }}-base.owl.gz --retry {{ project.import_group.mirror_retry_download }} --max-time {{ project.import_group.mirror_max_time_download }} && $(ROBOT) convert -i mirror/{{ ont.id }}-base.owl.gz -o $@.tmp.owl{%- if ont.make_base %} &&\
		$(ROBOT) remove -i $@.tmp.owl {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{iri}} {% endfor %}{% else %}--base-iri $(URIBASE)/{{ ont.id.upper() }} {% endif %}--axioms external --preserve-structure false --trim false -o $@.tmp.owl{% endif %} && mv $@.tmp.owl $@; fi
{%- else %}
mirror/{{ ont.id }}.owl: mirror/{{ ont.id }}.trigger
	if [ $(MIR) = true ] && [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then curl -L $(URIBASE)/{{ ont.id }}/{{ ont.id }}-base.owl --create-dirs -o mirror/{{ ont.id }}.owl --retry {{ project.import_group.mirror_retry_download }} --max-time {{ project.import_group.mirror_max_time_download }} && $(ROBOT) convert -i mirror/{{ ont.id }}.owl -o $@.tmp.owl && mv $@.tmp.owl $@; fi
{%- endif %}
{%- else %}
{%- if ont.use_gzipped %}
mirror/{{ ont.id }}.owl: mirror/{{ ont.id }}.trigger
	if [ $(MIR) = true ] && [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then curl -L $(URIBASE)/{{ ont.id }}.owl.gz --create-dirs -o mirror/{{ ont.id }}.owl.gz --retry {{ project.import_group.mirror_retry_download }} --max-time {{ project.import_group.mirror_max_time_download }} && $(ROBOT) convert -i mirror/{{ ont.id }}.owl.gz -o $@.tmp.owl{%- if ont.make_base %} && \
		$(ROBOT) remove -i $@.tmp.owl {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{iri}} {% endfor %}{% else %}--base-iri $(URIBASE)/{{ ont.id.upper() }} {% endif %}--axioms external --preserve-structure false --trim false -o $@.tmp.owl{% endif %} && mv $@.tmp.owl $@; fi
{%- else %}
mirror/{{ ont.id }}.owl: mirror/{{ ont.id }}.trigger
	if [ $(MIR) = true ] && [ $(IMP) = true ]{% if ont.is_large %} && [ $(IMP_LARGE) = true ]{% endif %}; then curl -L $(URIBASE)/{{ ont.id }}.owl --create-dirs -o mirror/{{ ont.id }}.owl --retry {{ project.import_group.mirror_retry_download }} --max-time {{ project.import_group.mirror_max_time_download }} && $(ROBOT) convert -i mirror/{{ ont.id }}.owl -o $@.tmp.owl{%- if ont.make_base %} && \
		$(ROBOT) remove -i $@.tmp.owl {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{iri}} {% endfor %}{% else %}--base-iri $(URIBASE)/{{ ont.id.upper() }} {% endif %}--axioms external --preserve-structure false --trim false -o $@.tmp.owl{% endif %} && mv $@.tmp.owl $@; fi
{%- endif %}
{%- endif %}
.PRECIOUS: mirror/{{ ont.id }}.owl
{% endfor -%}
{% if project.import_group.use_base_merging %}
ALL_MIRRORS = $(patsubst %, mirror/%.owl, $(IMPORTS))

mirror/merged.owl: $(ALL_MIRRORS)
	if [ $(MIR) = true ] && [ $(IMP) = true ]; then $(ROBOT) merge $(patsubst %, -i %, $^) -o $@; fi
.PRECIOUS: mirror/merged.owl
{% endif %}
{% endif %}

{% if project.subset_group is defined %}
# ----------------------------------------
# Subsets
# ----------------------------------------
subsets/%.tsv: subsets/%.owl
	$(ROBOT) query -f tsv -i $< -s ../sparql/labels.sparql $@
.PRECIOUS: subsets/%.tsv

subsets/%.owl: $(ONT).owl
	$(OWLTOOLS) $< --extract-ontology-subset --fill-gaps --subset $* -o $@.tmp.owl && mv $@.tmp.owl $@ &&\
	$(ROBOT) annotate --input $@ --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) -o $@.tmp.owl && mv $@.tmp.owl $@
.PRECIOUS: subsets/%.owl

{% if 'obo' in project.export_formats %}
subsets/%.obo: subsets/%.owl
	$(ROBOT) convert --input $< --check false -f obo $(OBO_FORMAT_OPTIONS) -o $@.tmp.obo && grep -v ^owl-axioms $@.tmp.obo > $@ && rm $@.tmp.obo
{% endif -%}
{% if 'ttl' in project.export_formats %}
subsets/%.ttl: subsets/%.owl
	$(ROBOT) convert --input $< --check false -f ttl -o $@.tmp.ttl && mv $@.tmp.ttl $@
{% endif -%}
{% if 'json' in project.export_formats %}
subsets/%.json: subsets/%.owl
	$(ROBOT) convert --input $< --check false -f json -o $@.tmp.json &&\
	jq -S 'walk(if type == "array" then sort else . end)' $@.tmp.json > $@ && rm $@.tmp.json
{% endif -%}

{% endif %}

# ----------------------------------------
# Release
# ----------------------------------------
# copy from staging area (this directory) to top-level
.PHONY: release
release: $(ONT).owl $(ONT).obo
	cp $^ $(RELEASEDIR) && cp imports/* $(RELEASEDIR)/imports

# ----------------------------------------
# Sparql queries: Q/C
# ----------------------------------------

# these live in the ../sparql directory, and have suffix -violation.sparql
# adding the name here will make the violation check live.
# NOTE: these will soon be phased out and replaced by robot-report

#  run all violation checks
SPARQL_VALIDATION_QUERIES = $(foreach V,$(SPARQL_VALIDATION_CHECKS),$(SPARQLDIR)/$(V)-violation.sparql)
sparql_test: $(SRC) catalog-v001.xml | $(REPORTDIR)
ifneq ($(SPARQL_VALIDATION_QUERIES),)
	$(ROBOT) verify  --catalog catalog-v001.xml -i $< --queries $(SPARQL_VALIDATION_QUERIES) -O $(REPORTDIR)
endif

# ----------------------------------------
# ROBOT report
# ----------------------------------------

$(REPORTDIR)/$(SRC)-obo-report.tsv: $(SRCMERGED) | $(REPORTDIR)
	$(ROBOT) report -i $< $(REPORT_LABEL) $(REPORT_PROFILE_OPTS) --fail-on $(REPORT_FAIL_ON) --print 5 -o $@

$(REPORTDIR)/%-obo-report.tsv: % | $(REPORTDIR)
	$(ROBOT) report -i $< $(REPORT_LABEL) $(REPORT_PROFILE_OPTS) --fail-on $(REPORT_FAIL_ON) --print 5 -o $@

# ----------------------------------------
# Sparql queries: Exports
# ----------------------------------------

SPARQL_EXPORTS_ARGS = $(foreach V,$(SPARQL_EXPORTS),-s $(SPARQLDIR)/$(V).sparql $(REPORTDIR)/$(V).tsv)
# This combines all into one single command

.PHONY: all_reports_onestep
all_reports_onestep: $(SRC) | $(REPORTDIR)
ifneq ($(SPARQL_EXPORTS_ARGS),)
	$(ROBOT) query -f tsv -i $< $(SPARQL_EXPORTS_ARGS)
endif

{%- if project.use_dosdps %}
# ----------------------------------------
# Patterns (experimental)
# ----------------------------------------

# Test patterns for schema compliance:

.PHONY: patterns
patterns: all_imports $(PATTERNDIR)/pattern.owl $(PATTERNDIR)/definitions.owl

.PHONY: pattern_clean
pattern_clean:
	echo "Not implemented"

.PHONY: pattern_schema_checks
pattern_schema_checks: update_patterns
	$(PATTERN_TESTER) $(PATTERNDIR)/dosdp-patterns/

#This command is a workaround for the absence of -N and -i in wget of alpine (the one ODK depend on now). It downloads all patterns specified in external.txt
.PHONY: update_patterns
update_patterns: .FORCE
	if [ $(PAT) = true ]; then rm -f $(PATTERNDIR)/dosdp-patterns/*.yaml.1 || true; fi
	if [ $(PAT) = true ] && [ -s $(PATTERNDIR)/dosdp-patterns/external.txt ]; then wget -i $(PATTERNDIR)/dosdp-patterns/external.txt --backups=1 -P $(PATTERNDIR)/dosdp-patterns; fi
	if [ $(PAT) = true ]; then rm -f $(PATTERNDIR)/dosdp-patterns/*.yaml.1 || true; fi


$(PATTERNDIR)/pattern.owl: pattern_schema_checks update_patterns
	if [ $(PAT) = true ]; then $(DOSDPT) prototype --obo-prefixes true --template=$(PATTERNDIR)/dosdp-patterns --outfile=$@; fi

individual_patterns_default := $(patsubst %.tsv, $(PATTERNDIR)/data/default/%.ofn, $(notdir $(wildcard $(PATTERNDIR)/data/default/*.tsv)))
pattern_term_lists_default := $(patsubst %.tsv, $(PATTERNDIR)/data/default/%.txt, $(notdir $(wildcard $(PATTERNDIR)/data/default/*.tsv)))

{% if project.pattern_pipelines_group is defined %}
{% for pipeline in project.pattern_pipelines_group.products %}
individual_patterns_{{ pipeline.id }} := $(patsubst %.tsv, $(PATTERNDIR)/data/{{ pipeline.id }}/%.ofn, $(notdir $(wildcard $(PATTERNDIR)/data/{{ pipeline.id }}/*.tsv)))
pattern_term_lists_{{ pipeline.id }} := $(patsubst %.tsv, $(PATTERNDIR)/data/{{ pipeline.id }}/%.txt, $(notdir $(wildcard $(PATTERNDIR)/data/{{ pipeline.id }}/*.tsv)))
{% endfor %}
{% endif %}

# Generating the individual pattern modules and merging them into definitions.owl
$(PATTERNDIR)/definitions.owl: prepare_patterns update_patterns dosdp_patterns_default {% if project.pattern_pipelines_group is defined %} {% for pipeline in project.pattern_pipelines_group.products %} dosdp_patterns_{{ pipeline.id }}{% endfor %}{% endif %}
	if [ $(PAT) = true ] && [ "${individual_patterns_names_default}" ] {% if project.pattern_pipelines_group is defined %} {% for pipeline in project.pattern_pipelines_group.products %} || [ "${individual_patterns_names_{{ pipeline.id }}}" ]{% endfor %}{% endif %} && [ $(PAT) = true ]; then $(ROBOT) merge $(addprefix -i , $(individual_patterns_default)) {% if project.pattern_pipelines_group is defined %} {% for pipeline in project.pattern_pipelines_group.products %} $(addprefix -i , $(individual_patterns_{{ pipeline.id }})){% endfor %}{% endif %} annotate --ontology-iri $(ONTBASE)/patterns/definitions.owl  --version-iri $(ONTBASE)/releases/$(TODAY)/patterns/definitions.owl --annotation owl:versionInfo $(VERSION) -o definitions.ofn && mv definitions.ofn $@; fi

individual_patterns_names_default := $(strip $(patsubst %.tsv,%, $(notdir $(wildcard $(PATTERNDIR)/data/default/*.tsv))))
dosdp_patterns_default: $(SRC) all_imports .FORCE
	if [ $(PAT) = true ] && [ "${individual_patterns_names_default}" ]; then $(DOSDPT) generate --catalog=catalog-v001.xml --infile=$(PATTERNDIR)/data/default/ --template=$(PATTERNDIR)/dosdp-patterns --batch-patterns="$(individual_patterns_names_default)" --ontology=$< {{ project.dosdp_tools_options }} --outfile=$(PATTERNDIR)/data/default; fi

{% if project.pattern_pipelines_group is defined %}
{% for pipeline in project.pattern_pipelines_group.products %}
individual_patterns_names_{{ pipeline.id }} := $(strip $(patsubst %.tsv,%, $(notdir $(wildcard $(PATTERNDIR)/data/{{ pipeline.id }}/*.tsv))))
dosdp_patterns_{{ pipeline.id }}: $(SRC) all_imports .FORCE
	if [ $(PAT) = true ] && [ "${individual_patterns_names_{{ pipeline.id }}}" ]; then $(DOSDPT) generate --catalog=catalog-v001.xml --infile=$(PATTERNDIR)/data/{{ pipeline.id }} --template=$(PATTERNDIR)/dosdp-patterns/ --batch-patterns="$(individual_patterns_names_{{ pipeline.id }})" --ontology=$< {{ pipeline.dosdp_tools_options }} --outfile=$(PATTERNDIR)/data/{{ pipeline.id }}; fi
{% endfor %}
{% endif %}

# Generating the seed file from all the TSVs. If Pattern generation is deactivated, we still extract a seed from definitions.owl
$(PATTERNDIR)/all_pattern_terms.txt: $(pattern_term_lists_default) {% if project.pattern_pipelines_group is defined %} {% for pipeline in project.pattern_pipelines_group.products %} $(pattern_term_lists_{{ pipeline.id }}){% endfor %}{% endif %} $(PATTERNDIR)/pattern_owl_seed.txt
	if [ $(PAT) = true ]; then cat $^ | sort | uniq > $@; else $(ROBOT) query --use-graphs true -f csv -i ../patterns/definitions.owl --query ../sparql/terms.sparql $@; fi

$(PATTERNDIR)/pattern_owl_seed.txt: $(PATTERNDIR)/pattern.owl
	if [ $(PAT) = true ]; then $(ROBOT) query --use-graphs true -f csv -i $< --query ../sparql/terms.sparql $@; fi

$(PATTERNDIR)/data/default/%.txt: $(PATTERNDIR)/dosdp-patterns/%.yaml $(PATTERNDIR)/data/default/%.tsv .FORCE
	if [ $(PAT) = true ]; then $(DOSDPT) terms --infile=$(word 2, $^) --template=$< --obo-prefixes=true --outfile=$@; fi

.PHONY: prepare_patterns
prepare_patterns:
	if [ $(PAT) = true ]; then touch $(PATTERNDIR)/data $(pattern_term_lists_default) {% if project.pattern_pipelines_group is defined %} {% for pipeline in project.pattern_pipelines_group.products %} $(pattern_term_lists_{{ pipeline.id }}){% endfor %}{% endif %}; fi
	if [ $(PAT) = true ]; then touch $(PATTERNDIR)/data $(individual_patterns_default) {% if project.pattern_pipelines_group is defined %} {% for pipeline in project.pattern_pipelines_group.products %} $(individual_patterns_{{ pipeline.id }}){% endfor %}{% endif %}; fi


{% if project.pattern_pipelines_group is defined -%}
{% for pipeline in project.pattern_pipelines_group.products %}
$(PATTERNDIR)/data/{{ pipeline.id }}/%.txt: $(PATTERNDIR)/dosdp-patterns/%.yaml $(PATTERNDIR)/data/{{ pipeline.id }}/%.tsv .FORCE
	if [ $(PAT) = true ]; then $(DOSDPT) terms --infile=$(word 2, $^) --template=$< --obo-prefixes=true --outfile=$@; fi
{% endfor %}
{% endif -%}
{% endif %}

# ----------------------------------------
# Release artefacts: export formats
# ----------------------------------------


{% for r in project.release_artefacts -%}
{% if r.startswith('custom-') %}{% set release = r | replace("custom-","") %}{% else %}{% set release = "$(ONT)-" ~ r %}{% endif -%}
{% if 'obo' in project.export_formats -%}
{{ release }}.obo: {{ release }}.owl
	$(ROBOT) convert --input $< --check false -f obo $(OBO_FORMAT_OPTIONS) -o $@.tmp.obo && grep -v ^owl-axioms $@.tmp.obo > $@ && rm $@.tmp.obo
{% endif -%}
{% if 'ttl' in project.export_formats -%}
{{ release }}.ttl: {{ release }}.owl
	$(ROBOT) annotate --input $< --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f ttl -o $@.tmp.ttl && mv $@.tmp.ttl $@
{% endif -%}
{% if 'json' in project.export_formats -%}
{{ release }}.json: {{ release }}.owl
	$(ROBOT) annotate --input $< --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f json -o $@.tmp.json &&\
	jq -S 'walk(if type == "array" then sort else . end)' $@.tmp.json > $@ && rm $@.tmp.json
{% endif -%}
{% endfor -%}

# We always want a base - even if it is not explicitly configured..
{% if 'base' not in project.release_artefacts -%}
{% if 'obo' in project.export_formats -%}
$(ONT)-base.obo: $(ONT)-base.owl
	$(ROBOT) convert --input $< --check false -f obo $(OBO_FORMAT_OPTIONS) -o $@.tmp.obo && grep -v ^owl-axioms $@.tmp.obo > $@ && rm $@.tmp.obo
{% endif -%}
{% if 'ttl' in project.export_formats -%}
$(ONT)-base.ttl: $(ONT)-base.owl
	$(ROBOT) annotate --input $< --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f ttl -o $@.tmp.ttl && mv $@.tmp.ttl $@
{% endif -%}
{% if 'json' in project.export_formats -%}
$(ONT)-base.json: $(ONT)-base.owl
	$(ROBOT) annotate --input $< --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f json -o $@.tmp.json &&\
	jq -S 'walk(if type == "array" then sort else . end)' $@.tmp.json > $@ && rm $@.tmp.json
{% endif -%}
{% endif -%}

# We always want a full release - even if it is not explicitly configured..
{% if 'full' not in project.release_artefacts -%}
{% if 'obo' in project.export_formats -%}
$(ONT)-full.obo: $(ONT)-full.owl
	$(ROBOT) convert --input $< --check false -f obo $(OBO_FORMAT_OPTIONS) -o $@.tmp.obo && grep -v ^owl-axioms $@.tmp.obo > $@ && rm $@.tmp.obo
{% endif -%}
{% if 'ttl' in project.export_formats -%}
$(ONT)-full.ttl: $(ONT)-full.owl
	$(ROBOT) annotate --input $< --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f ttl -o $@.tmp.ttl && mv $@.tmp.ttl $@
{% endif -%}
{% if 'json' in project.export_formats -%}
$(ONT)-full.json: $(ONT)-full.owl
	$(ROBOT) annotate --input $< --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f json -o $@.tmp.json &&\
	jq -S 'walk(if type == "array" then sort else . end)' $@.tmp.json > $@ && rm $@.tmp.json
{% endif -%}
{% endif -%}

# ----------------------------------------
# Release artefacts: main release artefacts
# ----------------------------------------

$(ONT).owl: $(ONT)-{{ project.primary_release }}.owl
	$(ROBOT) annotate --input $< --ontology-iri $(URIBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert -o $@.tmp.owl && mv $@.tmp.owl $@

{% if 'obo' in project.export_formats -%}
$(ONT).obo: $(ONT).owl
	$(ROBOT) convert --input $< --check false -f obo $(OBO_FORMAT_OPTIONS) -o $@.tmp.obo && grep -v ^owl-axioms $@.tmp.obo > $@ && rm $@.tmp.obo
{% endif -%}
{% if 'ttl' in project.export_formats -%}
$(ONT).ttl: $(ONT)-{{ project.primary_release }}.owl
	$(ROBOT) annotate --input $< --ontology-iri $(URIBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f ttl -o $@.tmp.ttl && mv $@.tmp.ttl $@
{% endif -%}
{% if 'json' in project.export_formats -%}
$(ONT).json: $(ONT)-{{ project.primary_release }}.owl
	$(ROBOT) annotate --input $< --ontology-iri $(URIBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f json -o $@.tmp.json && \
	jq -S 'walk(if type == "array" then sort else . end)' $@.tmp.json > $@ && rm $@.tmp.json
{% endif -%}

# -----------------------------------------------------
# Release artefacts: variants (base, full, simple, etc)
# -----------------------------------------------------
SHARED_ROBOT_COMMANDS = {% if project.remove_owl_nothing -%}remove --term owl:Nothing{% endif %}

$(ONTOLOGYTERMS): $(SRC) $(OTHER_SRC)
	touch $(ONTOLOGYTERMS) && \
	$(ROBOT) query --use-graphs true -f csv -i $< --query ../sparql/{{ project.id }}_terms.sparql $@

{% for format in project.export_formats %}
{% if project.gzip_main -%}
$(ONT).{{ format }}.gz: $(ONT).{{ format }}
	gzip -c $< > $@.tmp && mv $@.tmp $@
{% endif %}
{% endfor %}
{% if 'owl' not in project.export_formats %}
{% if project.gzip_main -%}
$(ONT).owl.gz: $(ONT).owl
	gzip -c $< > $@.tmp && mv $@.tmp $@
{% endif %}
$(ONT).owl: $(ONT)-{{ project.primary_release }}.owl
	cp $< $@
{% endif %}

# base: OTHER sources of interest, such as definitions owl
$(ONT)-base.owl: $(SRC) $(OTHER_SRC)
	$(ROBOT) remove --input $< --select imports --trim false \
		{% if project.use_dosdps or project.components is defined %}merge $(patsubst %, -i %, $(OTHER_SRC)) \
		{% endif %} $(SHARED_ROBOT_COMMANDS) annotate --link-annotation http://purl.org/dc/elements/1.1/type http://purl.obolibrary.org/obo/IAO_8000001 \
		--ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		{% if project.release_date -%} --annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@

# Full: The full artefacts with imports merged, reasoned
$(ONT)-full.owl: $(SRC) $(OTHER_SRC)
	$(ROBOT) merge --input $< \
		reason --reasoner {{ project.reasoner }} --equivalent-classes-allowed {{ project.allow_equivalents }} --exclude-tautologies {{ project.exclude_tautologies }} \
		relax \
		reduce -r {{ project.reasoner }} \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@

{% if 'non-classified' in project.release_artefacts or project.primary_release == 'non-classified' -%}
# foo-non-classified: (edit->imports-merged)
$(ONT)-non-classified.owl: $(SRC) $(OTHER_SRC)
	$(ROBOT) merge --input $< \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'simple' in project.release_artefacts or project.primary_release == 'simple' -%}
# foo-simple: (edit->reason,relax,reduce,drop imports, drop every axiom which contains an entity outside the "namespaces of interest")
# drop every axiom: filter --term-file keep_terms.txt --trim true
#	remove --select imports --trim false \

$(ONT)-simple.owl: $(SRC) $(OTHER_SRC) $(SIMPLESEED)
	$(ROBOT) merge --input $< $(patsubst %, -i %, $(OTHER_SRC)) \
		reason --reasoner {{ project.reasoner }} --equivalent-classes-allowed {{ project.allow_equivalents }} --exclude-tautologies {{ project.exclude_tautologies }} \
		relax \
		remove --axioms equivalent \
		relax \
		filter --term-file $(SIMPLESEED) --select "annotations ontology anonymous self" --trim true --signature true \
		reduce -r {{ project.reasoner }} \
		query --update ../sparql/inject-subset-declaration.ru \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'simple-non-classified' in project.release_artefacts or project.primary_release == 'simple-non-classified' -%}
# foo-simple-non-classified (edit->relax,reduce,drop imports, drop every axiom which contains an entity outside the "namespaces of interest") - aka the HPO use case, no reason.
# Should this be the non-classified ontology with the drop foreign axiom filter?
# Consider adding remove --term "http://www.geneontology.org/formats/oboInOwl#hasOBONamespace"

$(ONT)-simple-non-classified.owl: $(SRC) $(OTHER_SRC) $(ONTOLOGYTERMS)
	$(ROBOT) remove --input $< --select imports --trim true \
		merge  $(patsubst %, -i %, $(OTHER_SRC))  \
		remove --axioms equivalent \
		reduce -r {{ project.reasoner }} \
		filter --select ontology --term-file $(ONTOLOGYTERMS) --trim false \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'basic' in project.release_artefacts or project.primary_release == 'basic' -%}
# foo-basic: A version of -simple containing only relationships using relations on a configurable whitelist (default = BFO:0000050 (?)). See above (David comment) for explanation.
# removes any axioms that contains one of the ops that not in the whitelist file

$(ONT)-basic.owl: $(SRC) $(OTHER_SRC) $(SIMPLESEED) $(KEEPRELATIONS)
	$(ROBOT) merge --input $< $(patsubst %, -i %, $(OTHER_SRC)) \
		reason --reasoner {{ project.reasoner }} --equivalent-classes-allowed {{ project.allow_equivalents }} --exclude-tautologies {{ project.exclude_tautologies }} \
		relax \
		remove --axioms equivalent \
		remove --axioms disjoint \
		remove --term-file $(KEEPRELATIONS) --select complement --select object-properties --trim true \
		relax \
		filter --term-file $(SIMPLESEED) --select "annotations ontology anonymous self" --trim true --signature true \
		reduce -r {{ project.reasoner }} \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}
{% for r in project.release_artefacts %}
{%- if r.startswith('custom-') %}
{{ r | replace("custom-","")}}.owl:
	echo "ERROR: You have configured a custom release artefact ($@); this release artefact needs to be define in {{ project.id }}.Makefile!" && false
{% endif -%}
{% endfor -%}

# ----------------------------------------
# Debugging Tools
# ----------------------------------------

explain_unsat: $(SRC) 
	$(ROBOT) explain -i $(SRC) -M unsatisfiability --unsatisfiable random:10 --explanation $(TMPDIR)/$@.md

{%- if project.public_release != 'none'  %}
# ----------------------------------------
# GitHub release (HIGHLY experimental)
# ----------------------------------------

RELEASEFILES={% if project.public_release_assets is not none -%}{% for gha in project.public_release_assets %} {{ gha }}{% endfor -%}{% else -%}$(ASSETS){% endif %}
TAGNAME=v$(TODAY)
{% endif %}

{% if project.public_release == 'github_curl'  %}
USER=unknown
GH_ASSETS = $(patsubst %, $(TMPDIR)/gh_release_asset_%.txt, $(RELEASEFILES))
GITHUB_REPO={{ project.github_org }}/{{ project.repo }}

$(TMPDIR)/release_get.txt: | $(TMPDIR)
	curl -s https://api.github.com/repos/${GITHUB_REPO}/releases/tags/${TAGNAME} > $@

$(TMPDIR)/release_op.txt: $(TMPDIR)/release_get.txt | $(TMPDIR)
	$(eval RELEASEID=$(shell cat $(TMPDIR)/release_get.txt | jq '.id'))
	if ! [ "$(RELEASEID)" -eq "$(RELEASEID)" ] ; then \
		curl -s -X POST \
			https://api.github.com/repos/${GITHUB_REPO}/releases \
			-H 'Accept: */*' \
			-H 'Content-Type: application/json' \
			-u ${USER} \
			-d '{ "tag_name": "${TAGNAME}", "target_commitish": "master", "name": "${TAGNAME}", "body": "Ontology release ${TODAY}", "draft": false, "prerelease": false }' > $@; \
	else \
		cp $< $@; \
	fi;

$(TMPDIR)/gh_release_id.txt: $(TMPDIR)/release_op.txt | $(TMPDIR)
	echo $(shell cat $(TMPDIR)/release_op.txt | jq '.id') > $@;

$(TMPDIR)/gh_release_asset_%.txt: $(TMPDIR)/gh_release_id.txt % | $(TMPDIR)
	curl -X POST \
		"https://uploads.github.com/repos/${GITHUB_REPO}/releases/$(shell cat $(TMPDIR)/gh_release_id.txt)/assets?name=$*&label=$*" \
		--data-binary @$* \
		-u ${USER} \
		-H 'Accept: */*' \
		-H 'Cache-Control: no-cache' \
		-H 'Connection: keep-alive' \
		-H 'Content-Type: application/octet-stream' > $@

public_release: $(TMPDIR)/gh_release_id.txt $(GH_ASSETS) | $(TMPDIR)
{% endif %}
{%- if project.public_release == 'github_python'  %}
GITHUB_RELEASE_PYTHON=make-release-assets.py

.PHONY: public_release
public_release:
	ls -alt $(ASSETS)
	$(GITHUB_RELEASE_PYTHON) --release $(TAGNAME) $(RELEASEFILES)
{%- endif %}

.PHONY: validate_idranges
validate_idranges:
	amm $(SCRIPTSDIR)/validate_id_ranges.sc {{ project.id }}-idranges.owl

.PHONY: update_repo
update_repo:
	sh $(SCRIPTSDIR)/update_repo.sh
	
{% if project.documentation is not none %}
update_docs:
	mkdocs gh-deploy --config-file ../../mkdocs.yaml
{%- endif %}

include {{ project.id }}.Makefile
