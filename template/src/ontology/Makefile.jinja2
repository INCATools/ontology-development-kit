{#-

  Jinja2 Template for Makefile

  This file is a template that generates the Makefile.
  This comment will not be included.

  See: http://jinja.pocoo.org/docs/2.10/templates

-#}
# ----------------------------------------
# Makefile for {{ project.id }}
# Generated using ontology-development-kit
# ODK Version: {% if env is defined %}{{env['ODK_VERSION'] or "Unknown" }}{% else %}"Unknown"{% endif %}
# ----------------------------------------
# IMPORTANT: DO NOT EDIT THIS FILE. To override default make goals, use {{ project.id }}.Makefile instead

{{ project.custom_makefile_header }}

{%- if project.config_hash %}
# Fingerprint of the configuration file when this Makefile was last generated
CONFIG_HASH=                {{ project.config_hash }}
{% endif %}

# ----------------------------------------
# Standard Constants
# ----------------------------------------
# these can be overwritten on the command line

OBOBASE=                    http://purl.obolibrary.org/obo
URIBASE=                    {{ project.uribase }}
ONT=                        {{ project.id }}
ONTBASE=                    {{ project.uribase }}/{% if project.uribase_suffix is not none %}{{ project.uribase_suffix }}{% else %}{{ project.id }}{% endif %}
EDIT_FORMAT=                {{ project.edit_format|default('owl') }}
SRC =                       $(ONT)-edit.$(EDIT_FORMAT)
MAKE_FAST=                  $(MAKE) IMP=false PAT=false COMP=false MIR=false
CATALOG=                    {{ project.catalog_file }}
{% if project.use_context -%}
CONTEXT_FILE =              config/context.json
{% endif -%}
ROBOT=                      robot --catalog $(CATALOG){% if project.use_context %} --add-prefixes $(CONTEXT_FILE){% endif %}
REASONER=                   {{ project.reasoner }}
{# Kept for backwards compatibility with existing custom Makefiles -#}
{% if project.owltools_memory|length -%}
OWLTOOLS_MEMORY =           {{ project.owltools_memory }}
{% endif -%}
OWLTOOLS =                  {% if project.owltools_memory|length %}OWLTOOLS_MEMORY=$(OWLTOOLS_MEMORY) {% endif %}owltools --use-catalog
RELEASEDIR=                 ../..
DOCSDIR=                    ../../docs
REPORTDIR=                  reports
TEMPLATEDIR=                ../templates
TMPDIR=                     tmp
MIRRORDIR=                  mirror
IMPORTDIR=                  imports
SUBSETDIR=                  subsets
SCRIPTSDIR=                 ../scripts
UPDATEREPODIR=              target
SPARQLDIR =                 ../sparql
COMPONENTSDIR =             {{ project.components.directory|default('components') }}
{%- if project.robot_report.custom_profile %}
ROBOT_PROFILE =             profile.txt
{%- endif %}
REPORT_FAIL_ON =            {{ project.robot_report.fail_on|default('None') }}
REPORT_LABEL =              {% if project.robot_report.use_labels|default(true) %}-l true{% endif %}
REPORT_PROFILE_OPTS =       {% if project.robot_report.custom_profile %}--profile $(ROBOT_PROFILE){% endif %}
OBO_FORMAT_OPTIONS =        {{ project.obo_format_options }}
SPARQL_VALIDATION_CHECKS =  {% for x in project.robot_report.custom_sparql_checks|default(['owldef-self-reference', 'iri-range', 'label-with-iri', 'multiple-replaced_by']) %}{{ x }} {% endfor %}
SPARQL_EXPORTS =            {% for x in project.robot_report.custom_sparql_exports|default(['basic-report', 'class-count-by-prefix', 'edges', 'xrefs', 'obsoletes', 'synonyms']) %}{{ x }} {% endfor %}
ODK_VERSION_MAKEFILE =      {% if env is defined %}{{env['ODK_VERSION'] or "Unknown" }}{% else %}"Unknown"{% endif %}
RELAX_OPTIONS =             --include-subclass-of true

TODAY ?=                    $(shell date +%Y-%m-%d)
OBODATE ?=                  $(shell date +'%d:%m:%Y %H:%M')
VERSION=                    $(TODAY)
ANNOTATE_ONTOLOGY_VERSION = annotate -V $(ONTBASE)/releases/$(VERSION)/$@ --annotation owl:versionInfo $(VERSION)
ANNOTATE_CONVERT_FILE =     annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) convert -f {{ project.import_component_format|default('ofn') }} --output $@.tmp.owl && mv $@.tmp.owl $@
OTHER_SRC =                 {% if project.use_dosdps -%}$(PATTERNDIR)/definitions.owl {% endif -%}{% if project.components is defined -%}{% for component in project.components.products -%}$(COMPONENTSDIR)/{{ component.filename }} {% endfor -%}{% endif %}
ONTOLOGYTERMS =             $(TMPDIR)/ontologyterms.txt
EDIT_PREPROCESSED =         $(TMPDIR)/$(ONT)-preprocess.owl
{% if project.use_context and 'db' in project.export_formats -%}
CONTEXT_FILE_CSV =          $(TMPDIR)/context.csv
{%- endif -%}

{%- if project.use_dosdps %}
PATTERNDIR=                 ../patterns
PATTERN_TESTER=             dosdp validate -i
DOSDPT=                     dosdp-tools
PATTERN_RELEASE_FILES=      $(PATTERNDIR)/definitions.owl $(PATTERNDIR)/pattern.owl
{% endif %}

{%- if project.use_mappings %}
MAPPINGDIR=                 {% if project.sssom_mappingset_group is not none %}{{project.sssom_mappingset_group.directory|default("../mappings")}}{% endif %}
MAPPING_TESTER=             sssom validate
SSSOMPY=                    sssom
MAPPINGS=                   {% if project.sssom_mappingset_group is not none %}{%- for mapping in project.sssom_mappingset_group.products %}{{ mapping.id }} {% endfor %}{% endif %}
MAPPING_RELEASE_FILES=      $(foreach n,$(MAPPINGS), $(MAPPINGDIR)/$(n).sssom.tsv)
{% endif %}

{%- if project.use_translations %}
TRANSLATIONSDIR=            {% if project.babelon_translation_group is not none %}{{project.babelon_translation_group.directory|default("../translations")}}{% endif %}
BABELONPY=                  babelon -q
TRANSLATIONS_OWL=           {%- for translation in project.babelon_translation_group.products %}$(TRANSLATIONSDIR)/{{ translation.id }}.babelon.owl {% if translation.include_robot_template_synonyms %} $(TRANSLATIONSDIR)/{{ translation.id }}.synonyms.owl {% endif %}{% endfor %}
TRANSLATIONS_TSV=           {%- for translation in project.babelon_translation_group.products %}$(TRANSLATIONSDIR)/{{ translation.id }}-preprocessed.babelon.tsv {% endfor %}
TRANSLATION_FILES=			{%- if project.babelon_translation_group is not none %}{% if project.babelon_translation_group.release_merged_translations %}$(TRANSLATIONSDIR)/$(ONT)-all.babelon.tsv $(TRANSLATIONSDIR)/$(ONT)-all.babelon.json{% endif %}{% endif %}
{% endif %}

FORMATS = $(sort {% for format in project.export_formats %} {{ format }}{% endfor %} owl)
FORMATS_INCL_TSV = $(sort $(FORMATS) tsv)
RELEASE_ARTEFACTS = $(sort {% for release in project.release_artefacts %}{% if release.startswith('custom-') %}{{ release | replace("custom-","")}}{% else %}$(ONT)-{{ release }}{% endif %} {% endfor %})

ifeq ($(ODK_DEBUG),yes)
ODK_DEBUG_FILE = debug.log
SHELL = $(SCRIPTSDIR)/run-command.sh
endif

# ----------------------------------------
# Workflow control
# ----------------------------------------
# Set any of the following variables to false to completely disable the
# corresponding workflows.

# Refresh of mirrors (and all remote resources more generally)
MIR = true

# Re-generation of import modules
IMP = true

# Re-generation of "large" import modules
# Note that IMP=false takes precedence over IMP_LARGE=true, that is,
# IMP=false disables the generation of all import modules, large or not.
IMP_LARGE = true

# Re-generation of component modules
COMP = true

# Re-generation of pattern-derived files
PAT = true

# ----------------------------------------
# Top-level targets
# ----------------------------------------

.PHONY: .FORCE

.PHONY: all
all: all_odk

.PHONY: all_odk
all_odk: odkversion{% if project.config_hash %} config_check{% endif %} test custom_reports all_assets{% if project.release_diff %} release_diff{% endif %}

.PHONY: test
test: odkversion validate_idranges {% if project.use_dosdps %}dosdp_validation {% endif %}reason_test sparql_test robot_reports {% if project.robot_report.ensure_owl2dl_profile|default(true) %}$(REPORTDIR)/validate_profile_owl2dl_$(ONT).owl.txt{% endif %}
	echo "Finished running all tests successfully."

.PHONY: test
test_fast:
	$(MAKE_FAST) test

.PHONY: release_diff
release_diff: $(REPORTDIR)/release-diff.md

.PHONY: reason_test
reason_test: $(EDIT_PREPROCESSED)
	$(ROBOT) reason --input $< --reasoner $(REASONER) --equivalent-classes-allowed {{ project.allow_equivalents }} \
		--exclude-tautologies {{ project.exclude_tautologies }} --output test.owl && rm test.owl

.PHONY: odkversion
odkversion:
	@echo "ODK Makefile $(ODK_VERSION_MAKEFILE)"
	@odk-info --tools

{%- if project.config_hash %}
.PHONY: config_check
config_check:
	@if [ "$$(tr -d '\r' < $(ONT)-odk.yaml | sha256sum | cut -c1-64)" = "$(CONFIG_HASH)" ]; then \
		echo "Repository is up-to-date." ; else \
		echo "Your ODK configuration has changed since this Makefile was generated. You may need to run 'make update_repo'." ; fi
{% endif %}

$(TMPDIR) $(REPORTDIR) $(MIRRORDIR) $(IMPORTDIR) $(COMPONENTSDIR) $(SUBSETDIR):
	mkdir -p $@

# ----------------------------------------
# ODK-managed ROBOT plugins
# ----------------------------------------

# Make sure ROBOT knows where to find plugins
export ROBOT_PLUGINS_DIRECTORY=$(TMPDIR)/plugins

# Override this rule in {{ project.id }}.Makefile to install custom plugins
.PHONY: custom_robot_plugins
custom_robot_plugins:

{% if project.robot_plugins is defined %}
.PHONY: extra_robot_plugins
extra_robot_plugins: {% for plugin in project.robot_plugins.plugins %} $(ROBOT_PLUGINS_DIRECTORY)/{{ plugin.name }}.jar {% endfor %}
{% endif %}

# Install all ROBOT plugins to the runtime plugins directory
.PHONY: all_robot_plugins
all_robot_plugins: $(foreach plugin,$(notdir $(wildcard /tools/robot-plugins/*.jar)),$(ROBOT_PLUGINS_DIRECTORY)/$(plugin)) \
		   $(foreach plugin,$(notdir $(wildcard ../../plugins/*.jar)),$(ROBOT_PLUGINS_DIRECTORY)/$(plugin)) \
		   custom_robot_plugins {% if project.robot_plugins is defined %}extra_robot_plugins {% endif %} \

# Default rule to install plugins
$(ROBOT_PLUGINS_DIRECTORY)/%.jar:
	@mkdir -p $(ROBOT_PLUGINS_DIRECTORY)
	@if [ -f ../../plugins/$*.jar ]; then        \
		ln ../../plugins/$*.jar $@ ;         \
	elif [ -f /tools/robot-plugins/$*.jar ]; then \
		cp /tools/robot-plugins/$*.jar $@ ;  \
	fi

# Specific rules for supplementary plugins defined in configuration
{% if project.robot_plugins is defined %}{% for plugin in project.robot_plugins.plugins %}
$(ROBOT_PLUGINS_DIRECTORY)/{{ plugin.name }}.jar:
{%- if plugin.mirror_from %}
	curl -L -o $@ {{ plugin.mirror_from }}
{%- else %}
	echo "ERROR: No URL has been provided for this plugin; you must install it yourself by overwriting this rule in {{ project.id }}.Makefile!" && false
{% endif %}
{% endfor %}{% endif %}

# ----------------------------------------
# Release assets
# ----------------------------------------

MAIN_PRODUCTS = $(sort $(foreach r,$(RELEASE_ARTEFACTS), $(r)) $(ONT))
MAIN_GZIPPED  = {% if project.gzip_main %}$(foreach f,$(FORMATS), $(ONT).$(f).gz){% endif %}
MAIN_FILES    = $(foreach n,$(MAIN_PRODUCTS), $(foreach f,$(FORMATS), $(n).$(f))) $(MAIN_GZIPPED)
SRCMERGED     = $(TMPDIR)/merged-$(ONT)-edit.ofn

.PHONY: all_main
all_main: $(MAIN_FILES)

# ----------------------------------------
# Import assets
# ----------------------------------------

{% if project.import_group is defined %}
IMPORTS = {% for imp in project.import_group.products %} {{ imp.id }}{% endfor %}
{% else %}
IMPORTS =
{% endif %}
IMPORT_ROOTS = {% if project.import_group.use_base_merging %} $(IMPORTDIR)/merged_import{% else %}$(patsubst %, $(IMPORTDIR)/%_import, $(IMPORTS)){% endif %}
IMPORT_OWL_FILES = $(foreach n,$(IMPORT_ROOTS), $(n).owl)
{%- if project.import_group.export_obo %}
IMPORT_OBO_FILES = $(foreach n,$(IMPORT_ROOTS), $(n).obo)
IMPORT_FILES = $(IMPORT_OWL_FILES) $(IMPORT_OBO_FILES)

{% else %}
IMPORT_FILES = $(IMPORT_OWL_FILES)
{% endif %}

.PHONY: all_imports
all_imports: $(IMPORT_FILES)

# ----------------------------------------
# Subset assets
# ----------------------------------------

{% if project.subset_group is defined %}
SUBSETS = {% for x in project.subset_group.products %} {{ x.id }}{% endfor %}
{% else %}
SUBSETS =
{% endif %}
SUBSET_ROOTS = $(patsubst %, $(SUBSETDIR)/%, $(SUBSETS))
SUBSET_FILES = $(foreach n,$(SUBSET_ROOTS), $(foreach f,$(FORMATS_INCL_TSV), $(n).$(f)))

.PHONY: all_subsets
all_subsets: $(SUBSET_FILES)

# ----------------------------------------
# Mapping assets
# ----------------------------------------

{% if project.sssom_mappingset_group is defined %}
MAPPINGS = {% for x in project.sssom_mappingset_group.products %} {{ x.id }}{% endfor %}
{% if project.sssom_mappingset_group.released_products is defined %}
RELEASED_MAPPINGS = {% for x in project.sssom_mappingset_group.released_products %} {{ x.id }}{% endfor %}{% endif %}
{% else %}
MAPPINGS =
{% endif %}
MAPPING_FILES = $(foreach p, $(MAPPINGS), $(MAPPINGDIR)/$(p).sssom.tsv)
RELEASED_MAPPING_FILES = $(foreach p, $(RELEASED_MAPPINGS), $(MAPPINGDIR)/$(p).sssom.tsv)

.PHONY: all_mappings
all_mappings: $(MAPPING_FILES)


# ----------------------------------------
# QC Reports & Utilities
# ----------------------------------------

OBO_REPORT = {% for x in project.robot_report.report_on|default(["edit"]) %} {% if x=="edit" %}$(SRC){% else %}{{ x }}{% endif %}-obo-report{% endfor %}
ALIGNMENT_REPORT = {% for x in project.robot_report.report_on|default(["edit"]) %} {% if x =="edit" %}$(SRC){% else %}{{ x }}{% endif %}-align-report{% endfor %}
REPORTS = $(OBO_REPORT){% if project.robot_report.upper_ontology is defined and project.robot_report.upper_ontology %} $(ALIGNMENT_REPORT){% endif %}
REPORT_FILES = $(patsubst %, $(REPORTDIR)/%.tsv, $(REPORTS))

.PHONY: robot_reports
robot_reports: $(REPORT_FILES)

.PHONY: all_reports
all_reports: custom_reports robot_reports

# ----------------------------------------
# ROBOT OWL Profile checking
# ----------------------------------------

# The merge step is necessary to avoid undeclared entity violations.
$(REPORTDIR)/validate_profile_owl2dl_%.txt: % | $(REPORTDIR) $(TMPDIR)
	$(ROBOT) merge -i $< convert -f ofn -o $(TMPDIR)/validate.ofn
	$(ROBOT) validate-profile --profile DL -i $(TMPDIR)/validate.ofn -o $@ || { cat $@ && exit 1; }
.PRECIOUS: $(REPORTDIR)/validate_profile_owl2dl_%.txt

validate_profile_%: $(REPORTDIR)/validate_profile_owl2dl_%.txt
	echo "$* profile validation completed."

# ----------------------------------------
# Sparql queries: Q/C
# ----------------------------------------

# these live in the ../sparql directory, and have suffix -violation.sparql
# adding the name here will make the violation check live.

SPARQL_VALIDATION_QUERIES = $(foreach V,$(SPARQL_VALIDATION_CHECKS),$(SPARQLDIR)/$(V)-violation.sparql)

sparql_test: {% for x in project.robot_report.sparql_test_on|default(["edit"]) %} {% if x=="edit" %}$(SRCMERGED){% else %}{{ x }}{% endif %}{% endfor %} | $(REPORTDIR)
ifneq ($(SPARQL_VALIDATION_QUERIES),)
  {% for x in project.robot_report.sparql_test_on|default(["edit"]) -%}
  {%- if x=="edit" -%}
  {% set input = "$(SRCMERGED)"%}
  {%- else -%}
  {% set input = x %}
  {% endif %}
	$(ROBOT) verify -i {{ input }} --queries $(SPARQL_VALIDATION_QUERIES) -O $(REPORTDIR)
  {%- endfor %}
endif

# ----------------------------------------
# ROBOT report
# ----------------------------------------

$(REPORTDIR)/$(SRC)-obo-report.tsv: $(SRCMERGED) | $(REPORTDIR)
	$(ROBOT) report -i $< $(REPORT_LABEL) $(REPORT_PROFILE_OPTS) --fail-on $(REPORT_FAIL_ON) {% if project.robot_report.use_base_iris %}{% if project.namespaces is not none %}{% for iri in project.namespaces %}--base-iri {{ iri }} {% endfor -%}{% else %}--base-iri $(URIBASE)/{{ project.id.upper() }}_ --base-iri $(URIBASE)/{{ project.id }} {% endif -%}{% endif -%} --print 5 -o $@

$(REPORTDIR)/%-obo-report.tsv: % | $(REPORTDIR)
	$(ROBOT) report -i $< $(REPORT_LABEL) $(REPORT_PROFILE_OPTS) --fail-on $(REPORT_FAIL_ON) {% if project.robot_report.use_base_iris %}{% if project.namespaces is not none %}{% for iri in project.namespaces %}--base-iri {{ iri }} {% endfor %}{% else %}--base-iri $(URIBASE)/{{ project.id.upper() }}_ --base-iri $(URIBASE)/{{ project.id }} {% endif %}{% endif -%} --print 5 -o $@
{%- if project.robot_report.upper_ontology is defined and project.robot_report.upper_ontology %}

$(REPORTDIR)/$(SRC)-align-report.tsv: $(SRCMERGED) | $(REPORTDIR) all_robot_plugins
	$(ROBOT) odk:check-align -i $< --reasoner $(REASONER) \
		 --upper-ontology-iri {{ project.robot_report.upper_ontology }}
		 {%- if project.robot_report.use_base_iris %} \
		 {% if project.namespaces is not none %}{% for iri in project.namespaces %}--base-iri {{ iri }} \
		 {% endfor %}{% else %}--base-iri $(URIBASE)/{{ project.id.upper() }}_ --base-iri $(URIBASE)/{{ project.id }} \
		 {% endif %}{% else %} \
		 {% endif %}--report-output $@

$(REPORTDIR)/%-align-report.tsv: % | $(REPORTDIR) all_robot_plugins
	$(ROBOT) odk:check-align -i $< --reasoner $(REASONER) \
		 --upper-ontology-iri {{ project.robot_report.upper_ontology }}
		 {%- if project.robot_report.use_base_iris %} \
		 {% if project.namespaces is not none %}{% for iri in project.namespaces %}--base-iri {{ iri }} \
		 {% endfor %}{% else %}--base-iri $(URIBASE)/{{ project.id.upper() }}_ --base-iri $(URIBASE)/{{ project.id }} \
		 {% endif %}{% else %} \
		 {% endif %}--report-output $@
{%- endif %}

check_for_robot_updates:
{%- if project.robot_report.custom_profile %}	
	@cut -f2 "/tools/templates/src/ontology/profile.txt" | sort > $(TMPDIR)/sorted_tsv2.txt
	@cut -f2 "$(ROBOT_PROFILE)" | sort > $(TMPDIR)/sorted_tsv1.txt
	@comm -23 $(TMPDIR)/sorted_tsv2.txt $(TMPDIR)/sorted_tsv1.txt > $(TMPDIR)/missing.txt
	@echo "Missing tests:"
	@cat $(TMPDIR)/missing.txt
	@rm $(TMPDIR)/sorted_tsv1.txt $(TMPDIR)/sorted_tsv2.txt $(TMPDIR)/missing.txt $(TMPDIR)/report_profile_robot.txt
{%- else %}
	echo "You are not using a custom profile, so you are getting the joy of the latest ROBOT report!"
{% endif %}

# ----------------------------------------
# Release assets
# ----------------------------------------

ASSETS = \
  $(IMPORT_FILES) \
  $(MAIN_FILES) \{% if project.use_dosdps %}
  $(PATTERN_RELEASE_FILES) \{% endif %}{% if project.use_translations %}
  $(TRANSLATION_FILES) \{% endif %}
  $(REPORT_FILES) \
  $(SUBSET_FILES) \
  $(MAPPING_FILES)

RELEASE_ASSETS = \
  $(MAIN_FILES) {% if project.import_group is defined %}{% if project.import_group.release_imports %}$(IMPORT_FILES) {% endif %}{% endif %}\
  $(SUBSET_FILES){% if project.robot_report.release_reports %} \
  $(REPORT_FILES){% endif %}

.PHONY: all_assets
all_assets: $(ASSETS) {% if project.ensure_valid_rdfxml %}check_rdfxml_assets{% endif %}

.PHONY: show_assets
show_assets:
	echo $(ASSETS)
	du -sh $(ASSETS)

.PHONY: show_release_assets
show_release_assets:
	@echo $(RELEASE_ASSETS)

check_rdfxml_%: %
	@check-rdfxml {% if project.extra_rdfxml_checks %}--jena --rdflib{% endif %} $<

.PHONY: check_rdfxml_assets
check_rdfxml_assets: $(foreach product,$(MAIN_PRODUCTS),check_rdfxml_$(product).owl)

# ----------------------------------------
# Release Management
# ----------------------------------------

{% if 'basic' in project.release_artefacts or project.primary_release == 'basic' -%}
KEEPRELATIONS=keeprelations.txt
{% endif -%}

CLEANFILES=$(MAIN_FILES) $(SRCMERGED) $(EDIT_PREPROCESSED)
# This should be executed by the release manager whenever time comes to make a release.
# It will ensure that all assets/files are fresh, and will copy to release folder

.PHONY: prepare_release
prepare_release: all_odk
	rsync -R $(RELEASE_ASSETS) $(RELEASEDIR) &&\{% if project.sssom_mappingset_group is defined %}{% if project.sssom_mappingset_group.released_products is defined %}
	mkdir -p $(RELEASEDIR)/mappings && cp -rf $(RELEASED_MAPPING_FILES) $(RELEASEDIR)/mappings &&\{% endif %}{% endif %}{% if project.use_dosdps %}
	mkdir -p $(RELEASEDIR)/patterns && cp -rf $(PATTERN_RELEASE_FILES) $(RELEASEDIR)/patterns &&\{% endif %}
	rm -f $(CLEANFILES) &&\
	echo "Release files are now in $(RELEASEDIR) - now you should commit, push and make a release \
        on your git hosting site such as GitHub or GitLab"

.PHONY: prepare_release_fast
prepare_release_fast:
	$(MAKE) prepare_release IMP=false PAT=false MIR=false COMP=false

CURRENT_RELEASE=$(ONTBASE).owl

$(TMPDIR)/current-release.owl:
	wget $(CURRENT_RELEASE) -O $@

$(REPORTDIR)/release-diff.md: $(ONT).owl $(TMPDIR)/current-release.owl
	$(ROBOT) diff --labels true --left $(TMPDIR)/current-release.owl --right $(ONT).owl -f markdown -o $@

# ------------------------
# Imports: Seeding system 
# ------------------------

{% if project.import_group.scan_signature -%}
# pre_seed.txt contains all entities referenced from the -edit file
# and its components
PRESEED=$(TMPDIR)/pre_seed.txt
$(PRESEED): $(SRCMERGED)
	$(ROBOT) query --input $< --format --csv \
		       --query $(SPARQLDIR)/terms.sparql $@

{% endif -%}
$(SRCMERGED): $(EDIT_PREPROCESSED) $(OTHER_SRC)
	$(ROBOT) remove --input $< --select imports --trim false \
		 merge $(foreach src, $(OTHER_SRC), --input $(src)) \
		       --output $@

$(EDIT_PREPROCESSED): $(SRC)
	$(ROBOT) convert --input $< --format ofn --output $@

{% if 'basic' in project.release_artefacts or 'simple' in project.release_artefacts or 'simple-non-classified' in project.release_artefacts or project.primary_release == 'basic' or project.primary_release == 'simple-non-classified' or project.primary_release == 'simple' -%}
SIMPLESEED=$(TMPDIR)/simple_seed.txt

$(SIMPLESEED): $(SRCMERGED) $(ONTOLOGYTERMS)
	$(ROBOT) query -f csv -i $< --query ../sparql/simple-seed.sparql $@.tmp &&\
	cat $@.tmp $(ONTOLOGYTERMS) | sort | uniq >  $@ &&\
	echo "http://www.geneontology.org/formats/oboInOwl#SubsetProperty" >> $@ &&\
	echo "http://www.geneontology.org/formats/oboInOwl#SynonymTypeProperty" >> $@

{% endif -%}
{% if project.use_custom_import_module -%}
IMPORT_MODULE_TEMPLATE=$(TEMPLATEDIR)/external_import.tsv
IMPORT_MODULE_SIGNATURE=$(TMPDIR)/external_import_terms.txt
IMPORT_MODULE=$(IMPORTDIR)/external_import.owl
$(IMPORT_MODULE): $(IMPORT_MODULE_TEMPLATE) | $(TMPDIR)
	$(ROBOT) template --template $< {% if project.use_context %}--add-prefixes $(CONTEXT_FILE) {% endif %}\
 		--ontology-iri "$(ONTBASE)/external_import.owl" \
  		convert -f {{ project.import_component_format|default('ofn') }} \
  		--output $@

$(IMPORT_MODULE_SIGNATURE): $(IMPORT_MODULE) | $(TMPDIR)
	$(ROBOT) query -f csv -i $< --query ../sparql/terms.sparql $@.tmp &&\
	cat $@.tmp | sort | uniq >  $@

{% endif -%}

{% if project.import_group.scan_signature or project.use_custom_import_module -%}
# seed.txt contains all entities to import in addition to those defined
# in the individual _terms.txt files.
IMPORTSEED = $(TMPDIR)/seed.txt
$(IMPORTSEED):
{%-  if project.import_group.scan_signature %} $(PRESEED)
{%-    if project.use_dosdps %} $(TMPDIR)/all_pattern_terms.txt{% endif -%}
{%-  endif -%}
{%-if project.use_custom_import_module %} $(IMPORT_MODULE_SIGNATURE){% endif %} | $(TMPDIR)
	cat $^ | sort | uniq > $@

T_IMPORTSEED = --term-file $(IMPORTSEED)

{% endif -%}

{% if project.import_group is defined -%}
{% if project.import_group.strip_annotation_properties -%}
ANNOTATION_PROPERTIES={% for p in project.import_group.annotation_properties %}{{ p }} {% endfor %}

{% endif -%}
# ----------------------------------------
# Import modules
# ----------------------------------------
# Most ontologies are modularly constructed using portions of other ontologies
# These live in the imports/ folder
# This pattern uses ROBOT to generate an import module

ifeq ($(IMP),true)

{%   if project.import_group.use_base_merging -%}
ALL_TERMS = $(foreach imp, $(IMPORTS), $(IMPORTDIR)/$(imp)_terms.txt)

{%     if 'slme' == project.import_group.module_type -%}
$(IMPORTDIR)/merged_import.owl: $(MIRRORDIR)/merged.owl $(ALL_TERMS) \
				$(IMPORTSEED) | all_robot_plugins
	$(ROBOT) merge --input $<{% if project.import_group.exclude_iri_patterns is not none -%}
		 {%- for pattern in project.import_group.exclude_iri_patterns %} \
		 remove --select "{{ pattern }}"{% endfor %}{% endif %} \
		 extract $(foreach f, $(ALL_TERMS), --term-file $(f)) $(T_IMPORTSEED) \
		         --force true --copy-ontology-annotations false \
		         --individuals {{ project.import_group.slme_individuals }} \
		         --method {{ project.import_group.module_type_slme }} \{% if project.import_group.strip_annotation_properties %}
		 remove $(foreach p, $(ANNOTATION_PROPERTIES), --term $(p)) \
		        $(foreach f, $(ALL_TERMS), --term-file $(f)) $(T_IMPORTSEED) \
		        --select complement --select annotation-properties \{% endif %}
		 odk:normalize --base-iri {{ project.uribase }} \
		               --subset-decls true --synonym-decls true \
		               --merge-axioms true \
		 $(ANNOTATE_CONVERT_FILE)

{%     else -%}
$(IMPORTDIR)/merged_import.owl: $(MIRRORDIR)/merged.owl $(ALL_TERMS) \
				$(IMPORTSEED)
	@echo "ERROR: You have configured the merged import as a custom module;"
	@echo "       This rule needs to be overwritten in {{ project.id }}.Makefile!"
	@false

{%     endif -%}
{%   else %}{# !project.import_group.use_base_merging -#}

## Default module type ({{ project.import_group.module_type }})
{%     if 'slme' == project.import_group.module_type -%}
$(IMPORTDIR)/%_import.owl: $(MIRRORDIR)/%.owl $(IMPORTDIR)/%_terms.txt \
			   $(IMPORTSEED) | all_robot_plugins
	$(ROBOT) annotate --input $< --remove-annotations \
		 odk:normalize --add-source true \
		 extract --term-file $(IMPORTDIR)/$*_terms.txt $(T_IMPORTSEED) \
		         --force true --copy-ontology-annotations true \
		         --individuals {{ project.import_group.slme_individuals }} \
		         --method {{ project.import_group.module_type_slme }} \{% if project.import_group.strip_annotation_properties %}
		 remove $(foreach p, $(ANNOTATION_PROPERTIES), --term $(p)) \
		        --term-file $(IMPORTDIR)/$*_terms.txt $(T_IMPORTSEED) \
		        --select complement --select annotation-properties \{% endif %}
		 odk:normalize --base-iri {{ project.uribase }} \
		               --subset-decls true --synonym-decls true \
		               --merge-axioms true \
		 $(ANNOTATE_CONVERT_FILE)

{%     elif 'minimal' == project.import_group.module_type -%}
$(IMPORTDIR)/%_import.owl: $(MIRRORDIR)/%.owl $(IMPORTDIR)/%_terms.txt \
			   $(IMPORTSEED) | all_robot_plugins
	$(ROBOT) annotate --input $< --remove-annotations \
		 odk:normalize --add-source true \
		 extract --term-file $(IMPORTDIR)/$*_terms.txt $(T_IMPORTSEED) \
		         --force true --copy-ontology-annotations true \
		         --method BOT \
		 remove --base-iri $(OBOBASE)"/$(shell echo $* | tr a-z A-Z)_" \
		        --axioms external \
		        --preserve-structure false --trim false \
		 odk:normalize --base-iri {{ project.uribase }} \
		               --subset-decls true --synonym-decls true \
		               --merge-axioms true \
		 remove $(foreach p, $(ANNOTATION_PROPERTIES), --term $(p)) \
		        --term-file $(IMPORTDIR)/$*_terms.txt $(T_IMPORTSEED) \
		        --select complement \
		        --select "classes individuals annotation-properties" \
		 $(ANNOTATE_CONVERT_FILE)

{%     elif 'mirror' == project.import_group.module_type -%}
$(IMPORTDIR)/%_import.owl: $(MIRRORDIR)/%.owl | all_robot_plugins
	$(ROBOT) annotate --input $< --remove-annotations \
		 odk:normalize --base-iri {{ project.uribase }} \
		               --subset-decls true --synonym-decls true \
		               --merge-axioms true --add-source true \
		 $(ANNOTATE_CONVERT_FILE)

{%     elif 'filter' == project.import_group.module_type -%}
$(IMPORTDIR)/%_import.owl: $(MIRRORDIR)/%.owl $(IMPORTDIR)/%_terms.txt \
			   $(IMPORTSEED) | all_robot_plugins
	$(ROBOT) merge --input $< \
		 annotate --remove-annotations \
		 odk:normalize --add-source true \
		 remove --base-iri $(OBOBASE)"/$(shell echo $* | tr a-z A-Z)_" \
		        --axioms external \
		        --preserve-structure false --trim false \
		 remove $(foreach p, $(ANNOTATION_PROPERTIES), --term $(p)) \
		        --term-file $(IMPORTDIR)/$*_terms.txt $(T_IMPORTSEED) \
		        --select complement \
		 odk:normalize --base-iri {{ project.uribase }} \
		               --subset-decls true --synonym-decls true \
		               --merge-axioms true \
		 $(ANNOTATE_CONVERT_FILE)

{%     elif 'custom' == project.import_group.module_type -%}
$(IMPORTDIR)/%_import.owl: $(MIRRORDIR)/%.owl
	@echo "ERROR: You have configured the default module type to be custom;"
	@echo "       This rule needs to be overwritten in {{ project.id }}.Makefile!"
	@false

{%     endif -%}
.PRECIOUS: $(IMPORTDIR)/%_import.owl

{#
 # An import module of a different type than the default type needs its
 # own specific rule to override the pattern rule above.
 # A module that is marked as "large" (regardless of its type) also
 # needs its own rule, so that it can be selectively excluded when
 # running under IMP_LARGE=false.
 # We generate those rules here.
-#}
{%     for ont in project.import_group.special_products -%}
## Module for ontology: {{ ont.id }} ({{ ont.module_type }})
{%       if ont.is_large -%}
ifeq ($(IMP_LARGE),true)
{%       endif -%}
{%       if 'slme' == ont.module_type -%}
$(IMPORTDIR)/{{ ont.id }}_import.owl: $(MIRRORDIR)/{{ ont.id }}.owl $(IMPORTDIR)/{{ ont.id }}_terms.txt $(IMPORTSEED) | all_robot_plugins
	$(ROBOT) annotate --input $< --remove-annotations \
		 odk:normalize --add-source true \
		 extract --term-file $(IMPORTDIR)/{{ ont.id }}_terms.txt $(T_IMPORTSEED) \
		         --copy-ontology-annotations true --force true \
		         --individuals {{ ont.slme_individuals }} \
		         --method {{ ont.module_type_slme }} \{% if project.import_group.strip_annotation_properties %}
		 remove $(foreach p, $(ANNOTATION_PROPERTIES), --term $(p)) \{% for p in ont.annotation_properties %}
		        --term {{ p }} \{% endfor %}
		        --term-file $(IMPORTDIR)/{{ ont.id }}_terms.txt $(T_IMPORTSEED) \
		        --select complement --select annotation-properties \{% endif %}
		 odk:normalize --base-iri {{ project.uribase }} \
		               --subset-decls true --synonym-decls true \
		               --merge-axioms true \
		 $(ANNOTATE_CONVERT_FILE)
{%       elif 'filter' == ont.module_type -%}
$(IMPORTDIR)/{{ ont.id }}_import.owl: $(MIRRORDIR)/{{ ont.id }}.owl $(IMPORTDIR)/{{ ont.id }}_terms.txt $(IMPORTSEED) | all_robot_plugins
	$(ROBOT) annotate --input $< --remove-annotations \
		 odk:normalize --add-source true \
		 extract --term-file $(IMPORTDIR)/{{ ont.id }}_terms.txt $(T_IMPORTSEED) \
		         --copy-ontology-annotations true --force true --method BOT \
		 remove --axioms external --preserve-structure false --trim false \{% for iri in ont.base_iris %}
		        --base-iri {{ iri }} \{% endfor %}
		 remove $(foreach p, $(ANNOTATION_PROPERTIES), --term $(p)) \{% for p in ont.annotation_properties %}
		        --term {{ p }} \{% endfor %}
		        --term-file $(IMPORTDIR)/{{ ont.id }}_terms.txt $(T_IMPORTSEED) \
		        --select complement \
		 odk:normalize --base-iri {{ project.uribase }} \
		               --subset-decls true --synonym-decls true \
		               --merge-axioms true \
		 $(ANNOTATE_CONVERT_FILE)
{%       elif 'mirror' == ont.module_type -%}
$(IMPORTDIR)/{{ ont.id }}_import.owl: $(MIRRORDIR)/{{ ont.id }}.owl | all_robot_plugins
	$(ROBOT) annotate --input $< --remove-annotations \
		 odk:normalize --base-iri {{ project.uribase }} \
		               --subset-decls true --synonym-decls true \
		               --merge-axioms true --add-source true \
		 $(ANNOTATE_CONVERT_FILE)
{%       elif 'minimal' == ont.module_type -%}
$(IMPORTDIR)/{{ ont.id }}_import.owl: $(MIRRORDIR)/{{ ont.id }}.owl $(IMPORTDIR)/{{ ont.id }}_terms.txt $(IMPORTSEED) | all_robot_plugins
	$(ROBOT) annotate --input $< --remove-annotations \
		 odk:normalize --add-source true \
		 extract --term-file $(IMPORTDIR)/{{ ont.id }}_terms.txt $(T_IMPORTSEED) \
		         --copy-ontology-annotations true --force true --method BOT \
		 remove --axioms external --preserve-structure false --trim false \{% for iri in ont.base_iris %}
		        --base-iri {{ iri }} \{% endfor %}
		 odk:normalize --base-iri {{ project.uribase }} --subset-decls true \
		               --synonym-decls true --merge-axioms true \
		 remove $(foreach p, $(ANNOTATION_PROPERTIES), --term $(p)) \{% for p in ont.annotation_properties %}
		        --term {{ p }} \{% endfor %}
		        --term-file $(IMPORTDIR)/{{ ont.id }}_terms.txt $(T_IMPORTSEED) \
		        --select complement \
		        --select "classes individual annotation-properties" \
		 $(ANNOTATE_CONVERT_FILE)
{%       elif 'custom' -%}
$(IMPORTDIR)/{{ ont.id }}_import.owl: {% if 'no_mirror' != ont.mirror_type %}$(MIRRORDIR)/{{ ont.id }}.owl{% endif %}
	@echo "ERROR: You have configured {{ ont.id }} as a custom module;"
	@echo "       This rule needs to be overwritten in {{ project.id }}.Makefile!"
	@false
{%       endif -%}
{%       if ont.is_large -%}
endif # IMP_LARGE=true
{%       endif %}
{%     endfor -%}
{%   endif %}{# !use_base_merging -#}

{%   if project.import_group.export_obo -%}
# Convert any import module to OBO (this can be useful for spot-checks
# and diffs).
$(IMPORTDIR)/%_import.obo: $(IMPORTDIR)/%_import.owl
	$(ROBOT) convert --input $< --check false --format obo --output $@

{%   endif -%}
endif # IMP=true

.PHONY: refresh-imports
refresh-imports:
	$(MAKE) IMP=true MIR=true PAT=false IMP_LARGE=true clean all_imports

.PHONY: no-mirror-refresh-imports
no-mirror-refresh-imports:
	$(MAKE) --assume-new=$(SRC) \
		$(foreach imp,$(IMPORTS),--assume-new=$(IMPORTDIR)/$(imp)_terms.txt) \
		IMP=true MIR=false PAT=false IMP_LARGE=true all_imports

.PHONY: refresh-imports-excluding-large
refresh-imports-excluding-large:
	$(MAKE) IMP=true MIR=true PAT=false IMP_LARGE=false clean all_imports

.PHONY: refresh-%
refresh-%:
	$(MAKE) --assume-new=$(SRC) --assume-new=$(IMPORTDIR)/$*_terms.txt \
		IMP=true IMP_LARGE=true MIR=true PAT=false $(IMPORTDIR)/$*_import.owl

.PHONY: no-mirror-refresh-%
no-mirror-refresh-%:
	$(MAKE) --assume-new=$(SRC) --assume-new=$(IMPORTDIR)/$*_terms.txt \
		IMP=true IMP_LARGE=true MIR=false PAT=false $(IMPORTDIR)/$*_import.owl

{% endif %}{# !project.import_group is defined -#}

{% if project.components is not none -%}
# ----------------------------------------
# Components
# ----------------------------------------
# Some ontologies contain external and internal components. A component is included in the ontology in its entirety.

ifeq ($(COMP),true)
.PHONY: all_components
all_components: $(OTHER_SRC)

.PHONY: recreate-components
recreate-components:
	$(MAKE) {% for component in project.components.products %}--assume-new=$(TMPDIR)/stamp-component-{{ component.filename }} \
		{% endfor %}COMP=true IMP=false MIR=true PAT=true all_components

.PHONY: no-mirror-recreate-components
no-mirror-recreate-components:
	$(MAKE) {% for component in project.components.products %}--assume-new=$(TMPDIR)/stamp-component-{{ component.filename }} \
		{% endfor %}COMP=true IMP=false MIR=false PAT=true all_components

.PHONY: recreate-%
recreate-%:
	$(MAKE) --assume-new=$(TMPDIR)/stamp-component-$*.owl \
		COMP=true IMP=false MIR=true PAT=true $(COMPONENTSDIR)/$*.owl

.PHONY: no-mirror-recreate-%
no-mirror-recreate-%:
	$(MAKE) --assume-new=$(TMPDIR)/stamp-component-$*.owl \
		COMP=true IMP=false MIR=false PAT=true $(COMPONENTSDIR)/$*.owl

$(COMPONENTSDIR)/%.owl: $(TMPDIR)/stamp-component-%.owl | $(COMPONENTSDIR)
	test -f $@ || touch $@
.PRECIOUS: $(COMPONENTSDIR)/%.owl

$(TMPDIR)/stamp-component-%.owl: | $(TMPDIR)
	touch $@
.PRECIOUS: $(TMPDIR)/stamp-component-%.owl

{%   for component in project.components.products -%}
{%     if component.source is not none -%}
ifeq ($(MIR),true)
.PHONY: component-download-{{ component.filename }}
component-download-{{ component.filename }}: | $(TMPDIR)
	$(ROBOT) merge -I {{ component.source }} \{% if component.make_base %}
		 remove {% for iri in component.base_iris %}--base-iri {{ iri }} \
		        {% endfor %}--axioms external --preserve-structure false --trim false \{% endif %}
		 annotate --ontology-iri $(ONTBASE)/$@ \
		          $(ANNOTATE_ONTOLOGY_VERSION) --output $(TMPDIR)/$@.owl

$(COMPONENTSDIR)/{{ component.filename }}: component-download-{{ component.filename }} $(TMPDIR)/stamp-component-{{ component.filename }}
	@if cmp -s $(TMPDIR)/component-download-{{ component.filename }}.owl $(TMPDIR)/component-download-{{ component.filename }}.tmp.owl ; then \
	        echo "Component identical." ; \
	else \
	        echo "Component different, updating." && \
	        cp $(TMPDIR)/component-download-{{ component.filename }}.owl $(TMPDIR)/component-download-{{ component.filename }}.tmp.owl && \
	        $(ROBOT) annotate --input $(TMPDIR)/component-download-{{ component.filename }}.owl \
	                          --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
	                          --output $@ ; \
	fi
.PRECIOUS: $(COMPONENTSDIR)/{{ component.filename }}
endif # MIR=true

{%     elif component.use_template -%}
$(COMPONENTSDIR)/{{ component.filename }}:{% for template in component.templates %} $(TEMPLATEDIR)/{{ template }}{% endfor %} $(TMPDIR)/stamp-component-{{ component.filename }}
	$(ROBOT) template {% if component.template_options is not none %}{{ component.template_options }}{% endif %} \
		 {% for template in component.templates %}--template $(TEMPLATEDIR)/{{ template }} \
		 {% endfor %}$(ANNOTATE_CONVERT_FILE)
.PRECIOUS: $(COMPONENTSDIR)/{{ component.filename }}

{%     elif component.use_mappings -%}
$(COMPONENTSDIR)/{{ component.filename }}:{% for mapping in component.mappings %} $(MAPPINGDIR)/{{ mapping }}{% endfor %} $(TMPDIR)/stamp-component-{{ component.filename }} | all_robot_plugins
	$(ROBOT) --add-prefix 'sssom: https://w3id.org/sssom/' \
		 --add-prefix 'semapv: http://w3id.org/semapv/vocab/' \
		 sssom:inject {% for mapping in component.mappings %}--sssom $(MAPPINGDIR)/{{ mapping }} \
		              {% endfor %}--create --direct \
		 $(ANNOTATE_CONVERT_FILE)
.PRECIOUS: $(COMPONENTSDIR)/{{ component.filename }}

{%     endif -%}
{%   endfor -%}
endif # COMP=true
{% endif %}{# ! project.components is not none -#}
{% if project.import_group is defined -%}
# ----------------------------------------
# Mirroring upstream ontologies
# ----------------------------------------

ifeq ($(MIR),true)
{% for ont in project.import_group.products %}

## ONTOLOGY: {{ ont.id }}
{% if ont.description  -%}
## {{ ont.description }}
{% endif -%}

{%- if ont.mirror_type != 'no_mirror' -%}
.PHONY: mirror-{{ ont.id }}
.PRECIOUS: $(MIRRORDIR)/{{ ont.id }}.owl
{%- endif -%}

{%- if ont.is_large %}
ifeq ($(IMP_LARGE),true)
{%- endif %}
{%- if ont.mirror_from %}
mirror-{{ ont.id }}: | $(TMPDIR)
	$(ROBOT) {% if ont.make_base or 'base' == ont.mirror_type %}remove -I {{ ont.mirror_from }} {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{iri}} {% endfor %}{% else %}--base-iri $(OBOBASE)/{{ ont.id.upper() }} {% endif %} --axioms external --preserve-structure false --trim false{% else %}convert -I {{ ont.mirror_from }}{% endif %} -o $(TMPDIR)/$@.owl
{%- elif ont.use_base %}
{%- if ont.use_gzipped %}
mirror-{{ ont.id }}: | $(TMPDIR)
	curl -L $(OBOBASE)/{{ ont.id }}/{{ ont.id }}-base.owl.gz --create-dirs -o $(MIRRORDIR)/{{ ont.id }}-base.owl.gz --retry {{project.import_group.mirror_retry_download}} --max-time {{ project.import_group.mirror_max_time_download }} && \
	$(ROBOT) {% if ont.make_base or 'base' == ont.mirror_type %}remove -i $(MIRROR_DIR)/{{ ont.id }}-base.owl.gz {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{iri}} {% endfor %}{ else %}--base-iri $(OBOBASE)/{{ ont.id.upper() }} {% endif %} --axioms external --preserve-structure false --trim false{% else %}convert -i $(MIRRORDIR)/{{ ont.id }}-base.owl.gz{% endif %} -o $(TMPDIR)/$@.owl
{%- else %}
mirror-{{ ont.id }}: | $(TMPDIR)
	curl -L $(OBOBASE)/{{ ont.id }}/{{ ont.id }}-base.owl --create-dirs -o $(TMPDIR)/{{ ont.id }}-download.owl --retry {{ project.import_group.mirror_retry_download }} --max-time {{ project.import_group.mirror_max_time_download }} && \
	$(ROBOT) convert -i $(TMPDIR)/{{ ont.id }}-download.owl -o $(TMPDIR)/$@.owl
{%- endif %}
{%- else %}
{%- if ont.use_gzipped %}
mirror-{{ ont.id }}: | $(TMPDIR)
	curl -L $(OBOBASE)/{{ ont.id }}.owl.gz --create-dirs -o $(MIRRORDIR)/{{ ont.id }}.owl.gz --retry {{ project.import_group.mirror_retry_download }} --max-time {{ project.import_group.mirror_max_time_download }} && \
	$(ROBOT) {% if ont.make_base or 'base' == ont.mirror_type %}remove -i $(MIRRORDIR)/{{ ont.id }}.owl.gz {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{iri}} {% endfor %}{% else %}--base-iri $(OBOBASE)/{{ ont.id.upper() }} {% endif %}--axioms external --preserve-structure false --trim false{% else %}convert -i $(MIRRORDIR)/{{ ont.id }}.owl.gz{% endif %} -o $(TMPDIR)/$@.owl
{%- elif 'custom' == ont.mirror_type %}
$(MIRRORDIR)/{{ ont.id }}.owl: 
	echo "ERROR: You have configured your default mirror type to be custom; this behavior needs to be overwritten in {{ project.id }}.Makefile!" && false
{%- elif 'no_mirror' == ont.mirror_type -%}
## You have configured your default mirror type to no_mirror.
{%- else %}
mirror-{{ ont.id }}: | $(TMPDIR)
	curl -L $(OBOBASE)/{{ ont.id }}.owl --create-dirs -o $(TMPDIR)/{{ ont.id }}-download.owl --retry {{ project.import_group.mirror_retry_download }} --max-time {{ project.import_group.mirror_max_time_download }} && \
	$(ROBOT) {% if ont.make_base or 'base' == ont.mirror_type %}remove -i $(TMPDIR)/{{ ont.id }}-download.owl {% if ont.base_iris is not none %}{% for iri in ont.base_iris %}--base-iri {{iri}} {% endfor %}{% else %}--base-iri $(OBOBASE)/{{ ont.id.upper() }} {% endif %}--axioms external --preserve-structure false --trim false{% else %}convert -i $(TMPDIR)/{{ ont.id }}-download.owl{% endif %} -o $(TMPDIR)/$@.owl
{%- endif %}
{%- endif %}
{%- if ont.is_large %}
endif
{%- endif %}
{% endfor -%}
{% if project.import_group.use_base_merging %}
ALL_MIRRORS = $(patsubst %, $(MIRRORDIR)/%.owl, $(IMPORTS))
MERGE_MIRRORS = true

ifeq ($(MERGE_MIRRORS),true)
$(MIRRORDIR)/merged.owl: $(ALL_MIRRORS)
	$(ROBOT) merge $(patsubst %, -i %, $^) {% if project.import_group.annotate_defined_by %}--annotate-defined-by true{% endif %} {% if project.import_group.base_merge_drop_equivalent_class_axioms %}remove --axioms equivalent --preserve-structure false {% endif %}-o $@
.PRECIOUS: $(MIRRORDIR)/merged.owl
endif
{% endif %}

$(MIRRORDIR)/%.owl: mirror-% | $(MIRRORDIR)
	if [ -f $(TMPDIR)/mirror-$*.owl ]; then if cmp -s $(TMPDIR)/mirror-$*.owl $@ ; then echo "Mirror identical, ignoring."; else echo "Mirrors different, updating." &&\
		cp $(TMPDIR)/mirror-$*.owl $@; fi; fi

endif # MIR=true
{% endif %}

{% if project.subset_group is defined %}
# ----------------------------------------
# Subsets
# ----------------------------------------
$(SUBSETDIR)/%.tsv: $(SUBSETDIR)/%.owl
	$(ROBOT) export -i $< --include classes \
		        --header "ID [IRI]|LABEL" --format tsv --export $@
.PRECIOUS: $(SUBSETDIR)/%.tsv

$(SUBSETDIR)/%.owl: $(ONT).owl | $(SUBSETDIR) all_robot_plugins
	$(ROBOT) odk:subset -i $< --subset $* --fill-gaps true \
		 annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) -o $@
.PRECIOUS: $(SUBSETDIR)/%.owl

{% if 'obo' in project.export_formats %}
$(SUBSETDIR)/%.obo: $(SUBSETDIR)/%.owl
	$(ROBOT) convert --input $< --check false -f obo $(OBO_FORMAT_OPTIONS) -o $@.tmp.obo && grep -v ^owl-axioms $@.tmp.obo > $@ && rm $@.tmp.obo
{% endif -%}
{% if 'ttl' in project.export_formats %}
$(SUBSETDIR)/%.ttl: $(SUBSETDIR)/%.owl
	$(ROBOT) convert --input $< --check false -f ttl -o $@.tmp.ttl && mv $@.tmp.ttl $@
{% endif -%}
{% if 'json' in project.export_formats %}
$(SUBSETDIR)/%.json: $(SUBSETDIR)/%.owl
	$(ROBOT) convert --input $< --check false -f json -o $@.tmp.json &&\
	mv $@.tmp.json $@
{% endif -%}

{% endif %}

# ---------------------------------------------
# Sparql queries: Table exports / Query Reports
# ---------------------------------------------

SPARQL_EXPORTS_ARGS = $(foreach V,$(SPARQL_EXPORTS),-s $(SPARQLDIR)/$(V).sparql $(REPORTDIR)/$(V).tsv)
# This combines all into one single command

.PHONY: custom_reports
custom_reports: $(EDIT_PREPROCESSED) | $(REPORTDIR)
ifneq ($(SPARQL_EXPORTS_ARGS),)
	$(ROBOT) query -f tsv --use-graphs true -i $< $(SPARQL_EXPORTS_ARGS)
endif

{% if project.use_dosdps -%}
# ----------------------------------------
# DOSDP Templates/Patterns
# ----------------------------------------

ALL_PATTERN_FILES=$(wildcard $(PATTERNDIR)/dosdp-patterns/*.yaml)
ALL_PATTERN_NAMES=$(strip $(patsubst %.yaml,%, $(notdir $(wildcard $(PATTERNDIR)/dosdp-patterns/*.yaml))))

PATTERN_CLEAN_FILES=../patterns/all_pattern_terms.txt \
	$(DOSDP_OWL_FILES_DEFAULT) $(DOSDP_TERM_FILES_DEFAULT){% if project.pattern_pipelines_group is defined  -%}
	{% for pipeline in project.pattern_pipelines_group.products %} \
	$(DOSDP_OWL_FILES_{{ pipeline.id.upper() }}) $(DOSDP_TERM_FILES_{{ pipeline.id.upper() }}){% endfor %}{% endif %}

.PHONY: pattern_clean
pattern_clean:
	rm -f $(PATTERN_CLEAN_FILES)

ifeq ($(PAT),true)

.PHONY: patterns
patterns dosdp:
	echo "Validating all DOSDP templates"
	$(MAKE) dosdp_validation
	echo "Building $(PATTERNDIR)/definitions.owl"
	$(MAKE) $(PATTERNDIR)/pattern.owl $(PATTERNDIR)/definitions.owl

# DOSDP Template Validation

$(TMPDIR)/pattern_schema_checks: $(ALL_PATTERN_FILES) | $(TMPDIR)
	$(PATTERN_TESTER) $(PATTERNDIR)/dosdp-patterns/ && touch $@

.PHONY: pattern_schema_checks
pattern_schema_checks dosdp_validation: $(TMPDIR)/pattern_schema_checks

.PHONY: update_patterns
update_patterns: download_patterns
	if [ -n "$$(find $(TMPDIR) -type f -path '$(TMPDIR)/dosdp/*.yaml')" ]; then cp -r $(TMPDIR)/dosdp/*.yaml $(PATTERNDIR)/dosdp-patterns; fi

# This command is a workaround for the absence of -N and -i in wget of alpine (the one ODK depend on now).
# It downloads all patterns specified in external.txt
.PHONY: download_patterns
download_patterns:
	rm -f $(TMPDIR)/dosdp/*.yaml.1 || true
	if [ -s $(PATTERNDIR)/dosdp-patterns/external.txt ]; then wget -i $(PATTERNDIR)/dosdp-patterns/external.txt --backups=1 -P $(TMPDIR)/dosdp; fi
	rm -f $(TMPDIR)/dosdp/*.yaml.1 || true

$(PATTERNDIR)/dospd-patterns/%.yml: download_patterns
	if cmp -s $(TMPDIR)/dosdp-$*.yml $@ ; then echo "DOSDP templates identical."; else echo "DOSDP templates different, updating." &&\
		cp $(TMPDIR)/dosdp-$*.yml $@; fi


# DOSDP Template: Pipelines
# Each pipeline gets its own directory structure

# DOSDP default pipeline

DOSDP_TSV_FILES_DEFAULT = $(wildcard $(PATTERNDIR)/data/default/*.tsv)
DOSDP_PATTERN_NAMES_DEFAULT = $(strip $(patsubst %.tsv, %, $(notdir $(DOSDP_TSV_FILES_DEFAULT))))
DOSDP_OWL_FILES_DEFAULT = $(foreach name, $(DOSDP_PATTERN_NAMES_DEFAULT), $(PATTERNDIR)/data/default/$(name).ofn)
DOSDP_TERM_FILES_DEFAULT = $(foreach name, $(DOSDP_PATTERN_NAMES_DEFAULT), $(PATTERNDIR)/data/default/$(name).txt)
DOSDP_YAML_FILES_DEFAULT = $(foreach name, $(DOSDP_PATTERN_NAMES_DEFAULT), $(PATTERNDIR)/dosdp-patterns/$(name).yaml)

$(DOSDP_OWL_FILES_DEFAULT): $(EDIT_PREPROCESSED) $(DOSDP_TSV_FILES_DEFAULT) $(ALL_PATTERN_FILES)
	if [ "${DOSDP_PATTERN_NAMES_DEFAULT}" ]; then $(DOSDPT) generate --catalog=$(CATALOG) \
    --infile=$(PATTERNDIR)/data/default/ --template=$(PATTERNDIR)/dosdp-patterns --batch-patterns="$(DOSDP_PATTERN_NAMES_DEFAULT)" \
    --ontology=$< {{ project.dosdp_tools_options }} --outfile=$(PATTERNDIR)/data/default; fi

.PHONY: dosdp-docs-default
dosdp-docs-default: $(EDIT_PREPROCESSED) $(DOSDP_TSV_FILES_DEFAULT) $(DOSDP_YAML_FILES_DEFAULT)
	mkdir -p $(DOCSDIR)/patterns/default
	$(DOSDPT) docs {{ project.dosdp_tools_options }} --catalog=$(CATALOG) \
		       --ontology=$< \
		       --infile=$(PATTERNDIR)/data/default \
		       --template=$(PATTERNDIR)/dosdp-patterns \
		       --batch-patterns="$(DOSDP_PATTERN_NAMES_DEFAULT)" \
		       --outfile=$(DOCSDIR)/patterns/default{% if project.repo_url %} \
		       --data-location-prefix={{ project.repo_url }}/src/patterns/data/default{% elif project.github_org and project.repo %} \
		       --data-location-prefix=https://github.com/{{ project.github_org }}/{{ project.repo }}/tree/{{ project.git_main_branch }}/src/patterns/data/default{% endif %}

{% if project.pattern_pipelines_group is defined %}
{% for pipeline in project.pattern_pipelines_group.products %}
# DOSDP {{ pipeline.id }} pipeline

DOSDP_TSV_FILES_{{ pipeline.id.upper() }} = $(wildcard $(PATTERNDIR)/data/{{ pipeline.id }}/*.tsv)
DOSDP_PATTERN_NAMES_{{ pipeline.id.upper() }} = $(strip $(patsubst %.tsv, %, $(notdir $(DOSDP_TSV_FILES_{{ pipeline.id.upper() }}))))
DOSDP_OWL_FILES_{{ pipeline.id.upper() }} = $(foreach name, $(DOSDP_PATTERN_NAMES_{{ pipeline.id.upper() }}), $(PATTERNDIR)/data/{{ pipeline.id }}/$(name).ofn)
DOSDP_TERM_FILES_{{ pipeline.id.upper() }} = $(foreach name, $(DOSDP_PATTERN_NAMES_{{ pipeline.id.upper() }}), $(PATTERNDIR)/data/{{ pipeline.id }}/$(name).txt)
DOSDP_YAML_FILES_{{ pipeline.id.upper() }} = $(foreach name, $(DOSDP_PATTERN_NAMES_{{ pipeline.id.upper() }}), $(PATTERNDIR)/dosdp-patterns/$(name).yaml)

$(DOSDP_OWL_FILES_{{ pipeline.id.upper() }}): $(EDIT_PREPROCESSED) $(DOSDP_TSV_FILES_{{ pipeline.id.upper() }}) $(ALL_PATTERN_FILES)
	if [ "${DOSDP_PATTERN_NAMES_{{ pipeline.id.upper() }}}" ]; then $(DOSDPT) generate --catalog=$(CATALOG) \
    --infile=$(PATTERNDIR)/data/{{ pipeline.id }} --template=$(PATTERNDIR)/dosdp-patterns/ --batch-patterns="$(DOSDP_PATTERN_NAMES_{{ pipeline.id.upper() }})" \
    --ontology=$< {{ pipeline.dosdp_tools_options }} --outfile=$(PATTERNDIR)/data/{{ pipeline.id }}; fi

.PHONY: dosdp-docs-{{ pipeline.id }}
dosdp-docs-{{ pipeline.id }}: $(EDIT_PREPROCESSED) $(DOSDP_TSV_FILES_{{ pipeline.id.upper() }}) $(DOSDP_YAML_FILES_{{ pipeline.id.upper() }})
	mkdir -p $(DOCSDIR)/patterns/{{ pipeline.id }}
	$(DOSDPT) docs {{ pipeline.dosdp_tools_options }} --catalog=$(CATALOG) \
		       --ontology=$< \
		       --infile=$(PATTERNDIR)/data/{{ pipeline.id }} \
		       --template=$(PATTERNDIR)/dosdp-patterns \
		       --batch-patterns="$(DOSDP_PATTERN_NAMES_{{ pipeline.id.upper() }})" \
		       --outfile=$(DOCSDIR)/patterns/{{ pipeline.id }}{% if project.repo_url %} \
		       --data-location-prefix={{ project.repo_url }}/src/patterns/data/{{ pipeline.id }}{% elif project.github_org and project.repo %} \
		       --data-location-prefix=https://github.com/{{ project.github_org }}/{{ project.repo }}/tree/{{ project.git_main_branch }}/src/patterns/data/{{ pipeline.id }}{% endif %}
{% endfor -%}
{% endif -%}

# Generate template file seeds

## Generate template file seeds
$(PATTERNDIR)/data/default/%.txt: $(PATTERNDIR)/dosdp-patterns/%.yaml $(PATTERNDIR)/data/default/%.tsv
	$(DOSDPT) terms --infile=$(word 2, $^) --template=$< --obo-prefixes=true --outfile=$@

{% if project.pattern_pipelines_group is defined -%}
{% for pipeline in project.pattern_pipelines_group.products %}
$(PATTERNDIR)/data/{{ pipeline.id }}/%.txt: $(PATTERNDIR)/dosdp-patterns/%.yaml $(PATTERNDIR)/data/{{ pipeline.id }}/%.tsv
	$(DOSDPT) terms --infile=$(word 2, $^) --template=$< --obo-prefixes=true --outfile=$@
{% endfor %}
{% if project.pattern_pipelines_group.matches is iterable -%}{% for matches in project.pattern_pipelines_group.matches %}
dosdp-matches-{{ matches.id }}: {{ matches.ontology }} $(ALL_PATTERN_FILES)
	$(DOSDPT) query --ontology=$< --catalog=$(CATALOG) --reasoner=elk {{ matches.dosdp_tools_options }} \
    --batch-patterns="$(ALL_PATTERN_NAMES)" --template="$(PATTERNDIR)/dosdp-patterns" --outfile="$(PATTERNDIR)/data/{{ matches.id }}/"
{% endfor %}{% endif -%}
{% endif -%}

# Generating the seed file from all the TSVs.
$(TMPDIR)/all_pattern_terms.txt: $(DOSDP_TERM_FILES_DEFAULT) {% if project.pattern_pipelines_group is defined %} {% for pipeline in project.pattern_pipelines_group.products %} $(DOSDP_TERM_FILES_{{ pipeline.id.upper() }}){% endfor %}{% endif %} $(TMPDIR)/pattern_owl_seed.txt
	cat $^ | sort | uniq > $@

$(TMPDIR)/pattern_owl_seed.txt: $(PATTERNDIR)/pattern.owl
	$(ROBOT) query --use-graphs true -f csv -i $< --query ../sparql/terms.sparql $@

# Pattern pipeline main targets: the generated OWL files

# Create pattern.owl, an ontology of all DOSDP patterns
$(PATTERNDIR)/pattern.owl: $(ALL_PATTERN_FILES)
	$(DOSDPT) prototype --obo-prefixes true --template=$(PATTERNDIR)/dosdp-patterns --outfile=$@

# Generating the individual pattern modules and merging them into definitions.owl
$(PATTERNDIR)/definitions.owl: $(DOSDP_OWL_FILES_DEFAULT) {% if project.pattern_pipelines_group is defined %} {% for pipeline in project.pattern_pipelines_group.products %} $(DOSDP_OWL_FILES_{{ pipeline.id.upper() }}){% endfor %}{% endif %}
	if [ "${DOSDP_PATTERN_NAMES_DEFAULT}" ] {% if project.pattern_pipelines_group is defined %} {% for pipeline in project.pattern_pipelines_group.products %} || [ "${DOSDP_PATTERN_NAMES_{{ pipeline.id.upper() }}}" ]{% endfor %}{% endif %} && [ $(PAT) = true ]; then $(ROBOT) merge $(addprefix -i , $^) \
		annotate --ontology-iri $(ONTBASE)/patterns/definitions.owl  --version-iri $(ONTBASE)/releases/$(TODAY)/patterns/definitions.owl \
      --annotation owl:versionInfo $(VERSION) -o definitions.ofn && mv definitions.ofn $@; fi

else # PAT=false
# Even if pattern generation is disabled, we still extract a seed from definitions.owl
$(TMPDIR)/all_pattern_terms.txt: $(PATTERNDIR)/definitions.owl
	$(ROBOT) query --use-graphs true -f csv -i $< --query $(SPARQLDIR)/terms.sparql $@

dosdp_validation:
endif

{% endif -%}
{% if project.use_mappings -%}
# ----------------------------------------
# SSSOM Mapping Files
# ----------------------------------------

validate-sssom-%:
	tsvalid $(MAPPINGDIR)/$*.sssom.tsv --comment "#"
	sssom validate $(MAPPINGDIR)/$*.sssom.tsv

validate_mappings:
	$(MAKE_FAST) $(foreach n,$(MAPPINGS),validate-sssom-$(n))

normalize-sssom-%:
	sssom-cli --output $(MAPPINGDIR)/$*.sssom.tsv $(MAPPINGDIR)/$*.sssom.tsv

normalize_mappings:
	$(MAKE_FAST) $(foreach n,$(MAPPINGS),normalize-sssom-$(n))

{%   if project.sssom_mappingset_group is not none -%}
{%     for mapping in project.sssom_mappingset_group.products -%}
{%       if mapping.maintenance == "extract" -%}
{%         if project.sssom_mappingset_group.mapping_extractor == "sssom-py" -%}
$(TMPDIR)/{{ mapping.id }}.obographs.json: {{ mapping.source_file }}
	$(ROBOT) annotate --input $< \
		          --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		 convert --check false --format json --output $@

$(MAPPINGDIR)/{{ mapping.id }}.sssom.tsv: $(TMPDIR)/{{ mapping.id }}.obographs.json
	sssom parse $< -I obographs-json {{ mapping.sssom_tool_options }} -o $@

{%         elif project.sssom_mappingset_group.mapping_extractor == "sssom-java" -%}
$(MAPPINGDIR)/{{ mapping.id }}.sssom.tsv: {{ mapping.source_file }} | all_robot_plugins
	$(ROBOT) sssom:xref-extract --input $< --replace \
		                    --ignore-treat-xrefs --all-xrefs \
		                    --mapping-file $@

{%         endif -%}
{%       elif mapping.maintenance == "manual" -%}
# This mappingset is manually curated, so we only check that the file actually exists.
$(MAPPINGDIR)/{{ mapping.id }}.sssom.tsv:
	test -f $@

{%       elif mapping.maintenance == "merged" -%}
$(MAPPINGDIR)/{{ mapping.id }}.sssom.tsv:{% for source in mapping.source_mappings %} $(MAPPINGDIR)/{{ source }}.sssom.tsv{% endfor %}
	sssom-cli --output $@ $^

{%       elif mapping.maintenance == "mirror" -%}
ifeq ($(MIR),true)
$(MAPPINGDIR)/{{ mapping.id }}.sssom.tsv:
	wget "{{ mapping.mirror_from }}" -O $@
endif

{%       elif mapping.maintenance == "custom" -%}
$(MAPPINGDIR)/{{ mapping.id }}.sssom.tsv:
	@echo "ERROR: You have configured {{ mapping.id }} as a custom mapping set;"
	@echo "       This rule needs to be overwritten in {{ project.id }}.Makefile!"
	@false

{%       endif -%}
{%     endfor -%}
{%   endif -%}

{% endif %}{# !project.use_mappings -#}

{% if project.use_translations -%}
# ----------------------------------------
# Babelon Translation Files
# ----------------------------------------

{%- if project.babelon_translation_group is not none %}

TRANSLATIONS_ADAPTER={{ project.babelon_translation_group.oak_adapter|default('pronto:$(ONT).obo')  }}
TRANSLATIONS_ONTOLOGY={{ project.babelon_translation_group.translate_ontology|default('$(ONT).obo')  }}
TRANSLATE_PREDICATES={% for predicate_id in project.babelon_translation_group.predicates|default(['IAO:0000115', 'rdfs:label'], true) %}{{ predicate_id }} {% endfor %}

{% for translation in project.babelon_translation_group.products %}
{% if translation.maintenance == "mirror" %}
$(TRANSLATIONSDIR)/{{ translation.id }}.babelon.tsv:
	wget "{{ translation.mirror_babelon_from }}" -O $@
{% if translation.include_robot_template_synonyms %}
$(TRANSLATIONSDIR)/{{ translation.id }}.synonyms.tsv:
	wget "{{ translation.mirror_synonyms_from }}" -O $@
{% endif %}
{% else %}
# This mappingset is manually curated, so we only check that the file actually exists.
$(TRANSLATIONSDIR)/{{ translation.id }}.babelon.tsv:
	test -f $@
{% if translation.include_robot_template_synonyms %}
$(TRANSLATIONSDIR)/{{ translation.id }}.synonyms.tsv:
	test -f $@
{% endif %}
{% endif %}

$(TRANSLATIONSDIR)/{{ translation.id }}-preprocessed.babelon.tsv: $(TRANSLATIONS_ONTOLOGY) $(TRANSLATIONSDIR)/{{ translation.id }}.babelon.tsv{% if translation.auto_translate %}
	@if [ -z "$(OPENAI_API_KEY)" ]; then echo "OPENAI_API_KEY must be set as as part of the make command, e.g. sh run.sh make OPENAI_API_KEY=\"sk-123\" my_command" && exit 1; fi{% endif %}
	$(BABELONPY) prepare-translation $(TRANSLATIONSDIR)/{{ translation.id }}.babelon.tsv \
		--oak-adapter $(TRANSLATIONS_ADAPTER) \
		--language-code {{ translation.language|default('en') }} \
		$(foreach n,$(TRANSLATE_PREDICATES), --field $(n)) \
		--output-source-changed $(TRANSLATIONSDIR)/{{ translation.id }}-changed.babelon.tsv  \
		--output-not-translated $(TRANSLATIONSDIR)/{{ translation.id }}-not-translated.babelon.tsv \
		--include-not-translated {{ translation.include_not_translated|default('false')|lower }} \
		--update-translation-status {{ translation.update_translation_status|default('true')|lower }} \
		--drop-unknown-columns {{ translation.drop_unknown_columns|default('true')|lower }} \
		-o $@{% if translation.auto_translate %}
	echo "Warning: By default, the toolkit employs LLM-mediated translations using the OpenAI API. This default may change at any time"
	echo "Warning: Never store API keys or other secrets in Makefiles or scripts you have in version control."
	export OPENAI_API_KEY="$(OPENAI_API_KEY)" &&\
	$(BABELONPY) translate $(TRANSLATIONSDIR)/{{ translation.id }}-not-translated.babelon.tsv -o $(TRANSLATIONSDIR)/{{ translation.id }}-translated.babelon.tsv
	$(BABELONPY) merge $(TRANSLATIONSDIR)/{{ translation.id }}-preprocessed.babelon.tsv $(TRANSLATIONSDIR)/{{ translation.id }}-translated.babelon.tsv -o $@
	{%- endif %}

{% endfor %}
{%- endif %}

$(TRANSLATIONSDIR)/%.synonyms.owl: $(TRANSLATIONSDIR)/%.synonyms.tsv
	$(ROBOT) template --template $< \
		annotate \
			--ontology-iri $(ONTBASE)/translations/$*.synonyms.owl \
			-V $(ONTBASE)/releases/$(VERSION)/translations/$*.synonyms.owl \
			--annotation owl:versionInfo $(VERSION) \
		convert -f owl --output $@
.PRECIOUS: $(TRANSLATIONSDIR)/%.synonyms.owl

$(TRANSLATIONSDIR)/%.babelon.owl: $(TRANSLATIONSDIR)/%-preprocessed.babelon.tsv
	$(BABELONPY) convert $< --output-format owl -o $@.tmp
	$(ROBOT) merge -i $@.tmp \
		annotate \
			--ontology-iri $(ONTBASE)/translations/$*.babelon.owl \
			-V $(ONTBASE)/releases/$(VERSION)/translations/$*.babelon.owl \
			--annotation owl:versionInfo $(VERSION) \
		convert -f owl --output $@
	@rm $@.tmp
.PRECIOUS: $(TRANSLATIONSDIR)/%.babelon.owl

$(TRANSLATIONSDIR)/$(ONT)-all.babelon.tsv: $(TRANSLATIONS_TSV)
	$(BABELONPY) merge $^ -o $@

$(TRANSLATIONSDIR)/%.babelon.json: $(TRANSLATIONSDIR)/%.babelon.tsv
	$(BABELONPY) convert $< --output-format json -o $@

{% endif -%}
# ----------------------------------------
# Release artefacts: export formats
# ----------------------------------------


{% for r in project.release_artefacts -%}
{% if r.startswith('custom-') %}{% set release = r | replace("custom-","") %}{% else %}{% set release = "$(ONT)-" ~ r %}{% endif -%}
{% if 'obo' in project.export_formats -%}
{{ release }}.obo: {{ release }}.owl
	$(ROBOT) convert --input $< --check false -f obo $(OBO_FORMAT_OPTIONS) -o $@.tmp.obo && grep -v ^owl-axioms $@.tmp.obo > $@ && rm $@.tmp.obo
{% endif -%}
{% if 'ttl' in project.export_formats -%}
{{ release }}.ttl: {{ release }}.owl
	$(ROBOT) annotate --input $< --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f ttl -o $@.tmp.ttl && mv $@.tmp.ttl $@
{% endif -%}
{% if 'json' in project.export_formats -%}
{{ release }}.json: {{ release }}.owl
	$(ROBOT) annotate --input $< --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f json -o $@.tmp.json &&\
		mv $@.tmp.json $@
{% endif -%}
{% endfor -%}

{% if 'db' in project.export_formats -%}
{% if project.use_context -%}
$(CONTEXT_FILE_CSV): $(CONTEXT_FILE) | $(TMPDIR)
	context2csv < $< > $@
{% endif -%}

%.db: %.owl{% if project.use_context %} $(CONTEXT_FILE_CSV){% endif %}
	@rm -f $*.db $*-relation-graph.tsv.gz .template.db .template.db.tmp
	semsql make $*.db{% if project.use_context %} -P $(CONTEXT_FILE_CSV){% endif %}
	@rm -f $*-relation-graph.tsv.gz .template.db .template.db.tmp
	@test -f $*.db || (echo "SQLite/SemSQL generation failed" && exit 1)
{% endif -%}

# ----------------------------------------
# Release artefacts: main release artefacts
# ----------------------------------------

$(ONT).owl: $(ONT)-{{ project.primary_release }}.owl
	$(ROBOT) annotate --input $< --ontology-iri $(URIBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert -o $@.tmp.owl && mv $@.tmp.owl $@

{% if 'obo' in project.export_formats -%}
$(ONT).obo: $(ONT).owl
	$(ROBOT) convert --input $< --check false -f obo $(OBO_FORMAT_OPTIONS) -o $@.tmp.obo && grep -v ^owl-axioms $@.tmp.obo > $@ && rm $@.tmp.obo
{% endif -%}
{% if 'ttl' in project.export_formats -%}
$(ONT).ttl: $(ONT).owl
	$(ROBOT) annotate --input $< --ontology-iri $(URIBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f ttl -o $@.tmp.ttl && mv $@.tmp.ttl $@
{% endif -%}
{% if 'json' in project.export_formats -%}
$(ONT).json: $(ONT).owl
	$(ROBOT) annotate --input $< --ontology-iri $(URIBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		convert --check false -f json -o $@.tmp.json &&\
		mv $@.tmp.json $@
{% endif -%}

# -----------------------------------------------------
# Release artefacts: variants (base, full, simple, etc)
# -----------------------------------------------------
SHARED_ROBOT_COMMANDS = {% if project.remove_owl_nothing -%}remove --term owl:Nothing{% endif %}

$(ONTOLOGYTERMS): $(SRCMERGED)
	$(ROBOT) query -f csv -i $< --query ../sparql/{{ project.id }}_terms.sparql $@

{% for format in project.export_formats -%}
{% if project.gzip_main -%}
$(ONT).{{ format }}.gz: $(ONT).{{ format }}
	gzip -c $< > $@.tmp && mv $@.tmp $@
{% endif -%}
{% endfor -%}
{% if 'owl' not in project.export_formats -%}
{% if project.gzip_main -%}
$(ONT).owl.gz: $(ONT).owl
	gzip -c $< > $@.tmp && mv $@.tmp $@
{% endif -%}
$(ONT).owl: $(ONT)-{{ project.primary_release }}.owl
	cp $< $@
{% endif -%}

# ROBOT pipeline that merges imports, including components.
ROBOT_RELEASE_IMPORT_MODE={% if project.use_edit_file_imports -%}$(ROBOT) merge --input $< {% if project.import_group is defined and project.import_group.use_base_merging is false and project.import_group.annotate_defined_by %}--annotate-defined-by true{% endif -%}{% else -%}
$(ROBOT) remove --input $< --select imports --trim false {% if project.use_dosdps or project.components is defined -%}
merge $(patsubst %, -i %, $(OTHER_SRC)) {% endif %}{% if project.import_group is defined -%}
merge $(patsubst %, -i %, $(IMPORT_FILES)){% endif %}{% endif %}

# ROBOT pipeline that removes imports, then merges components. This is for release artefacts that start from "base"
ROBOT_RELEASE_IMPORT_MODE_BASE=$(ROBOT) remove --input $< --select imports --trim false {% if project.use_dosdps or project.components is defined -%}
merge $(patsubst %, -i %, $(OTHER_SRC)) {% endif %}

{% if 'base' in project.release_artefacts or project.primary_release == 'base' -%}
# base: A version of the ontology that does not include any externally imported axioms.
$(ONT)-base.owl: $(EDIT_PREPROCESSED) $(OTHER_SRC) $(IMPORT_FILES)
	$(ROBOT_RELEASE_IMPORT_MODE) \{% if project.release_use_reasoner %}
	reason --reasoner $(REASONER) --equivalent-classes-allowed {{ project.allow_equivalents }} --exclude-tautologies {{ project.exclude_tautologies }} --annotate-inferred-axioms {{ project.release_annotate_inferred_axioms|default('false')|lower }} \{% if project.release_materialize_object_properties is defined and project.release_materialize_object_properties %}
	materialize {% for iri in project.release_materialize_object_properties %}--term {{iri}} {% endfor %} \{% endif %}{% endif %}
	relax $(RELAX_OPTIONS) \{% if project.release_use_reasoner %}
	reduce -r $(REASONER) \{% endif %}
	remove {% if project.namespaces is not none %}{% for iri in project.namespaces %}--base-iri {{iri}} {% endfor %}{% else %}--base-iri $(URIBASE)/{{ project.id.upper() }} {% endif %}--axioms external --preserve-structure false --trim false \
	$(SHARED_ROBOT_COMMANDS) \
	annotate --link-annotation http://purl.org/dc/elements/1.1/type http://purl.obolibrary.org/obo/IAO_8000001 \
		--ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		{% if project.release_date -%} --annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'baselite' in project.release_artefacts or project.primary_release == 'baselite' -%}
# baselite: All the axioms as they are editted by the editors, excluding reasoning. This is currently the same as "base". Only to be used experimentally (may disappear in future ODK releases).
$(ONT)-baselite.owl: $(EDIT_PREPROCESSED) $(OTHER_SRC)
	$(ROBOT_RELEASE_IMPORT_MODE_BASE) \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		{% if project.release_date -%} --annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'full' in project.release_artefacts or project.primary_release == 'full' -%}
# Full: The full artefacts with imports merged, reasoned.
$(ONT)-full.owl: $(EDIT_PREPROCESSED) $(OTHER_SRC) $(IMPORT_FILES)
	$(ROBOT_RELEASE_IMPORT_MODE) \
		reason --reasoner $(REASONER) --equivalent-classes-allowed {{ project.allow_equivalents }} --exclude-tautologies {{ project.exclude_tautologies }} \{% if project.release_materialize_object_properties is defined and project.release_materialize_object_properties %}
		materialize {% for iri in project.release_materialize_object_properties %}--term {{iri}} {% endfor %} \{% endif %}
		relax $(RELAX_OPTIONS) \
		reduce -r $(REASONER) \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'non-classified' in project.release_artefacts or project.primary_release == 'non-classified' -%}
# foo-non-classified: (edit->imports-merged)
$(ONT)-non-classified.owl: $(EDIT_PREPROCESSED) $(OTHER_SRC) $(IMPORT_FILES)
	$(ROBOT_RELEASE_IMPORT_MODE) \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'simple' in project.release_artefacts or project.primary_release == 'simple' -%}
# foo-simple: (edit->reason,relax,reduce,drop imports, drop every axiom which contains an entity outside the "namespaces of interest")
# drop every axiom: filter --term-file keep_terms.txt --trim true
#	remove --select imports --trim false
$(ONT)-simple.owl: $(EDIT_PREPROCESSED) $(OTHER_SRC) $(SIMPLESEED) $(IMPORT_FILES) | all_robot_plugins
	$(ROBOT_RELEASE_IMPORT_MODE) \{% if project.release_use_reasoner %}
		reason --reasoner $(REASONER) --equivalent-classes-allowed {{ project.allow_equivalents }} --exclude-tautologies {{ project.exclude_tautologies }} --annotate-inferred-axioms {{ project.release_annotate_inferred_axioms|default('false')|lower }} \{% endif %}
		relax $(RELAX_OPTIONS) \
		remove --axioms equivalent \
		filter --term-file $(SIMPLESEED) --select "annotations ontology anonymous self" --trim true --signature true \{% if project.release_use_reasoner %}
		reduce -r $(REASONER) \{% endif %}
		odk:normalize --base-iri {{ project.uribase }} --subset-decls true --synonym-decls true --merge-axioms true \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'simple-non-classified' in project.release_artefacts or project.primary_release == 'simple-non-classified' %}
# foo-simple-non-classified (edit->relax,reduce,drop imports, drop every axiom which contains an entity outside the "namespaces of interest") - aka the HPO use case, no reason.
# Should this be the non-classified ontology with the drop foreign axiom filter?
# Consider adding remove --term "http://www.geneontology.org/formats/oboInOwl#hasOBONamespace"
$(ONT)-simple-non-classified.owl: $(EDIT_PREPROCESSED) $(OTHER_SRC) $(SIMPLESEED) $(IMPORT_FILES)
	$(ROBOT_RELEASE_IMPORT_MODE_BASE) \
		remove --axioms equivalent \{% if project.release_use_reasoner %}
		reduce -r $(REASONER) \{% endif %}
		filter --select ontology --term-file $(SIMPLESEED) --trim false \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'international' in project.release_artefacts or project.primary_release == 'international' -%}
# international: A variant of the primary_release, but with multi-language support.
$(ONT)-international.owl: $(ONT).owl $(TRANSLATIONS_OWL)
	$(ROBOT) merge $(patsubst %, -i %, $^) \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) \
		{% if project.release_date -%} --annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}

{% if 'basic' in project.release_artefacts or project.primary_release == 'basic' %}
# foo-basic: A version of -simple containing only relationships using relations on a configurable whitelist (default = BFO:0000050 (?)). 
# See above (David comment) for explanation.
# removes any axioms that contains one of the ops that not in the whitelist file
$(ONT)-basic.owl: $(EDIT_PREPROCESSED) $(OTHER_SRC) $(SIMPLESEED) $(KEEPRELATIONS) $(IMPORT_FILES)
	$(ROBOT_RELEASE_IMPORT_MODE) \{% if project.release_use_reasoner %}
		reason --reasoner $(REASONER) --equivalent-classes-allowed {{ project.allow_equivalents }} --exclude-tautologies {{ project.exclude_tautologies }} --annotate-inferred-axioms {{ project.release_annotate_inferred_axioms|default('false')|lower }} \{% endif %}
		relax $(RELAX_OPTIONS) \
		remove --axioms equivalent \
		remove --axioms disjoint \
		remove --term-file $(KEEPRELATIONS) --select complement --select object-properties --trim true \
		filter --term-file $(SIMPLESEED) --select "annotations ontology anonymous self" --trim true --signature true \
		reduce -r $(REASONER) \
		$(SHARED_ROBOT_COMMANDS) annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) {% if project.release_date -%}--annotation oboInOwl:date "$(OBODATE)" {% endif -%}--output $@.tmp.owl && mv $@.tmp.owl $@
{% endif -%}
{% for r in project.release_artefacts %}
{%- if r.startswith('custom-') %}
{{ r | replace("custom-","")}}.owl:
	echo "ERROR: You have configured a custom release artefact ($@); this release artefact needs to be define in {{ project.id }}.Makefile!" && false
{% endif -%}
{% endfor -%}

# ----------------------------------------
# Debugging Tools
# ----------------------------------------

explain_unsat: $(EDIT_PREPROCESSED) 
	$(ROBOT) explain -i $< -M unsatisfiability --unsatisfiable random:10 --explanation $(TMPDIR)/$@.md

{%- if project.public_release != 'none'  %}
# ----------------------------------------
# GitHub release (HIGHLY experimental)
# ----------------------------------------

RELEASEFILES={% if project.public_release_assets is not none -%}{% for gha in project.public_release_assets %} {{ gha }}{% endfor -%}{% else -%}$(ASSETS){% endif %}
TAGNAME=v$(TODAY)
{% endif %}

{% if project.public_release == 'github_curl'  %}
USER=unknown
GH_ASSETS = $(patsubst %, $(TMPDIR)/gh_release_asset_%.txt, $(RELEASEFILES))
GITHUB_REPO={{ project.github_org }}/{{ project.repo }}

$(TMPDIR)/release_get.txt: | $(TMPDIR)
	curl -s https://api.github.com/repos/${GITHUB_REPO}/releases/tags/${TAGNAME} > $@

$(TMPDIR)/release_op.txt: $(TMPDIR)/release_get.txt | $(TMPDIR)
	$(eval RELEASEID=$(shell cat $(TMPDIR)/release_get.txt | jq '.id'))
	if ! [ "$(RELEASEID)" -eq "$(RELEASEID)" ] ; then \
		curl -s -X POST \
			https://api.github.com/repos/${GITHUB_REPO}/releases \
			-H 'Accept: */*' \
			-H 'Content-Type: application/json' \
			-u ${USER} \
			-d '{ "tag_name": "${TAGNAME}", "target_commitish": "master", "name": "${TAGNAME}", "body": "Ontology release ${TODAY}", "draft": false, "prerelease": false }' > $@; \
	else \
		cp $< $@; \
	fi;

$(TMPDIR)/gh_release_id.txt: $(TMPDIR)/release_op.txt | $(TMPDIR)
	echo $(shell cat $(TMPDIR)/release_op.txt | jq '.id') > $@;

$(TMPDIR)/gh_release_asset_%.txt: $(TMPDIR)/gh_release_id.txt % | $(TMPDIR)
	curl -X POST \
		"https://uploads.github.com/repos/${GITHUB_REPO}/releases/$(shell cat $(TMPDIR)/gh_release_id.txt)/assets?name=$*&label=$*" \
		--data-binary @$* \
		-u ${USER} \
		-H 'Accept: */*' \
		-H 'Cache-Control: no-cache' \
		-H 'Connection: keep-alive' \
		-H 'Content-Type: application/octet-stream' > $@

public_release: $(TMPDIR)/gh_release_id.txt $(GH_ASSETS) | $(TMPDIR)
{% endif %}
{%- if project.public_release == 'github_python'  %}
GITHUB_RELEASE_PYTHON=make-release-assets.py

.PHONY: public_release
public_release:
	ls -alt $(ASSETS)
	$(GITHUB_RELEASE_PYTHON) --release $(TAGNAME) $(RELEASEFILES)
{% else  %}

RELEASE_ASSETS_AFTER_RELEASE=$(foreach n,$(RELEASE_ASSETS), $(RELEASEDIR)/$(n)) $(foreach n,$(RELEASED_MAPPINGS), $(RELEASEDIR)/mappings/$(n).sssom.tsv)
GHVERSION=v$(VERSION)

.PHONY: public_release
public_release:
	@test $(GHVERSION)
	ls -alt $(RELEASE_ASSETS_AFTER_RELEASE)
	gh release create $(GHVERSION) --title "$(VERSION) Release" --draft $(RELEASE_ASSETS_AFTER_RELEASE) --generate-notes

{%- endif %}

# ----------------------------------------
# General Validation
# ----------------------------------------
TSV=
ALL_TSV_FILES={% if project.use_dosdps %}{% if project.pattern_pipelines_group is defined -%}{% for pipeline in project.pattern_pipelines_group.products %}$(DOSDP_TSV_FILES_{{ pipeline.id | upper }}) {% endfor %}{% endif %}$(DOSDP_TSV_FILES_DEFAULT){% endif %}

validate-tsv: $(TSV) | $(TMPDIR)
	for FILE in $< ; do \
		tsvalid $$FILE > $(TMPDIR)/validate.txt; \
		if [ -s $(TMPDIR)/validate.txt ]; then cat $(TMPDIR)/validate.txt && exit 1; fi ; \
	done

validate-all-tsv: $(ALL_TSV_FILES)
	$(MAKE) validate-tsv TSV="$^"

# ----------------------------------------
# Editors Utilities
# ----------------------------------------

{% if 'obo' in project.edit_format -%}
.PHONY: normalize_obo_src
normalize_obo_src: $(SRC) | all_robot_plugins
	$(ROBOT) odk:normalize -i $< --merge-axioms true \
		 convert -o $(TMPDIR)/NORM.tmp.obo && \
	mv $(TMPDIR)/NORM.tmp.obo $(SRC)
{%- endif %}

.PHONY: normalize_src
normalize_src: $(SRC)
	$(ROBOT) convert -i $< -f {% if 'obo' == project.edit_format %}obo --check false{% else %}ofn{% endif %} -o $(TMPDIR)/normalise && mv $(TMPDIR)/normalise $<

.PHONY: validate_idranges
validate_idranges:
	if [ -f {{ project.id }}-idranges.owl ]; then dicer-cli {{ project.id }}-idranges.owl ; fi

# Deprecated: Use 'sh run.sh odk.py update' without using the Makefile.
.PHONY: update_repo
update_repo:
	odk.py update
	
{% if project.documentation is not none %}
update_docs:
	mkdocs gh-deploy --config-file ../../mkdocs.yaml
{%- endif %}

# Note to future generations: computing the real path relative to the
# current directory is a way to ensure we only clean up directories that
# are located below the current directory, regardless of the contents of
# the *DIR variables.
.PHONY: clean
clean:{% if project.use_dosdps %}
	$(MAKE) pattern_clean{%- endif %}
	for dir in $(MIRRORDIR) $(TMPDIR) $(UPDATEREPODIR) ; do      \
		reldir=$$(realpath --relative-to=$$(pwd) $$dir) ;    \
		case $$reldir in .*|"") ;; *) rm -rf $$reldir/* ;; esac \
	done
	rm -f $(CLEANFILES)

.PHONY: help
help:
	@echo "$$data"

define data
Usage: [IMAGE=(odklite|odkfull)] [ODK_DEBUG=yes] sh run.sh make [(IMP|MIR|IMP_LARGE|PAT)=(false|true)] command

----------------------------------------
	Command reference
----------------------------------------

Core commands:
* prepare_release:	Run the entire release pipeline. Use make IMP=false prepare_release to avoid rerunning the imports
* prepare_release_fast:	Run the entire release pipeline without refreshing imports, recreating components or recompiling patterns.
* update_repo:		Update the ODK repository setup using the config file {{ project.id }}-odk.yaml (DEPRECATED)
* test:			Running all validation tests
* test_fast:		Runs the test suite, but without updating imports or components
* odkversion:		Show the current version of the ODK Makefile and ROBOT.
* clean:		Delete all temporary files
* help:			Print ODK Usage information
* public_release:	Uploads the release file to a release management system, such as GitHub releases. Must be configured.

{% if project.import_group is defined %}
Imports management:
* refresh-imports:			Refresh all imports and mirrors.
* recreate-components:			Recreate all components.
* no-mirror-refresh-imports:		Refresh all imports without downloading mirrors.
* refresh-imports-excluding-large:	Refresh all imports and mirrors, but skipping the ones labelled as 'is_large'.
* refresh-%:				Refresh a single import, i.e. refresh-go will refresh 'imports/go_import.owl'.
* no-mirror-refresh-%:			Refresh a single import without updating the mirror, i.e. refresh-go will refresh 'imports/go_import.owl'.
* mirror-%:				Refresh a single mirror.
{% endif %}{% if project.use_dosdps %}
DOSDP templates
* dosdp:			Run the DOSDP patterns pipeline: Run tests, then build OWL files from the tables.
* patterns:			Alias of the 'dosdp' command
* pattern_clean:		Delete all temporary pattern files
* dosdp_validation:		Run all validation checks on DOSDP template files and tables
* pattern_schema_checks:	Alias of the 'dosdp_validation' command
* update_patterns:		Pull updated patterns listed in dosdp-patterns/external.txt
* dosdp-matches-%:		Run the DOSDP matches/query pipeline as configured in your {{ project.id }}-odk.yaml file.
* dosdp-docs-%:                 Generate the documentation for a given DOSDP pipeline.
{% endif %}
Editor utilities:
* validate_idranges:	Make sure your ID ranges file is formatted correctly
* normalize_src:	Load and save your {{ project.id }}-edit file after you to make sure its serialised correctly{% if 'obo' in project.edit_format %}
* normalize_obo_src:	Load and save your {{ project.id }}-edit.obo file after you to merge duplicate annotation assertions{% endif %}
* explain_unsat:	If you have unsatisfiable classes, this command will create a markdown file (tmp/explain_unsat.md) which will explain all your unsatisfiable classes
* validate-all-tsv:	Check all your tsv files for possible problems in syntax. Use ALL_TSV_FILES variable to list files
* validate-tsv:		Check a tsv file for syntactic problems with tsvalid. Use TSV variable to pass filepath, e.g. make TSV=../my.tsv validate-tsv.
* release_diff:	Create a diff between the current release and the new release

Additional build commands (advanced users)
* all:			Run the entire pipeline (like prepare_release), but without copying the release files to the release directory.
* all_subsets:		Build all subsets
* custom_reports:	Generate all custom sparql reports you have configured in your {{ project.id }}-odk.yaml file.
* all_assets:		Build all assets
* show_assets:		Print a list of all assets that would be build by the release pipeline
* all_mappings:		Update all SSSOM mapping sets

Additional QC commands (advanced users)
* robot_reports:	Run all configured ROBOT reports
* validate_profile_%:	Run an OWL2 DL profile validation check, for example validate_profile_{{ project.id }}-edit.owl.
* reason_test: Run a basic reasoning test

Examples: 
* sh run.sh make IMP=false prepare_release
* sh run.sh make test

Tricks:
* To forcefully rebuild a target even if nothing has changed, either
  invoke the 'clean' target (which will wipe out intermediate files)
  or touch a file that your target depends on (typically the -edit file).
* Use the IMAGE parameter to the run.sh script to use a different image like odklite
* Use ODK_DEBUG=yes sh run.sh make ... to print information about timing and debugging

Updating the repository:
(to apply changes to the ODK configuration or switch to a newer ODK version)
* sh run.sh update_repo

endef
export data

include {{ project.id }}.Makefile
